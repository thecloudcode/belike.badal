{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.54250812531\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train = pd.read_csv(\"/home/saurabhg/PuertoSergo/train.csv\")\n",
    "test = pd.read_csv(\"/home/saurabhg/PuertoSergo/test.csv\")\n",
    "print(time.time()-start)\n",
    " \n",
    "y = train['target']\n",
    "testid= test['id'].values\n",
    "\n",
    "train.drop(['id','target'],axis=1,inplace=True)\n",
    "test.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.061964035\n",
      "('Init Shape: ', (595212, 39))\n",
      "('After Shape: ', (595212, 136))\n",
      "('Init Shape: ', (892816, 39))\n",
      "('After Shape: ', (892816, 136))\n",
      "77.6997139454\n"
     ]
    }
   ],
   "source": [
    "### Drop calc\n",
    "\n",
    "start = time.time()\n",
    "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(unwanted, axis=1)  \n",
    "test = test.drop(unwanted, axis=1)\n",
    "\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "#train['ps_reg_03_square'] = train['ps_reg_03']*train['ps_reg_03']\n",
    "#test['ps_reg_03_square'] = test['ps_reg_03']*test['ps_reg_03']\n",
    "#train['ps_car_14_square'] = train['ps_car_14']*train['ps_car_14']\n",
    "#test['ps_car_14_square'] = test['ps_car_14']*test['ps_car_14']\n",
    "def recon(reg):\n",
    "    integer = int(np.round((40*reg)**2)) \n",
    "    for a in range(32):\n",
    "        if (integer - a) % 31 == 0:\n",
    "            A = a\n",
    "    M = (integer - A)//31\n",
    "    return A, M\n",
    "train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "train['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "train['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "test['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "test['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "\n",
    "print(time.time()-start)\n",
    "\n",
    "\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "d_skew = train.skew(axis=0)\n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c: #standard arithmetic\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "    return df\n",
    "\n",
    "def multi_transform(df, map=transform_df):\n",
    "    print('Init Shape: ', df.shape)\n",
    "    p = Pool(cpu_count())\n",
    "    df = p.map(transform_df, np.array_split(df, cpu_count()))\n",
    "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    p.close(); p.join()\n",
    "    print('After Shape: ', df.shape)\n",
    "    return df\n",
    "\n",
    "train = multi_transform(train)\n",
    "test = multi_transform(test)\n",
    "\n",
    "print(time.time()-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "\n",
    "@jit\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_lgb(act, preds):\n",
    "#    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(act, preds)\n",
    "    return 'gini', gini_score,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 5000\n",
    "OPTIMIZE_ROUNDS = True\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "f_cats = [f for f in train.columns if \"_cat\" in f]\n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_estimators = 1000\n",
    "folds = KFold(n_splits=n_splits, shuffle=True, random_state=1) \n",
    "imp_df = np.zeros((len(train.columns), n_splits))\n",
    "oof = np.empty(len(train))\n",
    "sub_preds = np.zeros((len(test),n_splits))\n",
    "increase = False\n",
    "np.random.seed(0)\n",
    "params = {'eta': 0.025, 'max_depth': 4, \n",
    "          'subsample': 0.9, 'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':100,\n",
    "          'alpha':10,\n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.261915\tvalid_1's gini: 0.247047\n",
      "[200]\ttraining's gini: 0.295073\tvalid_1's gini: 0.271944\n",
      "[300]\ttraining's gini: 0.316138\tvalid_1's gini: 0.282308\n",
      "[400]\ttraining's gini: 0.330736\tvalid_1's gini: 0.285496\n",
      "[500]\ttraining's gini: 0.343112\tvalid_1's gini: 0.286666\n",
      "Early stopping, best iteration is:\n",
      "[487]\ttraining's gini: 0.341676\tvalid_1's gini: 0.286977\n",
      "Fold  1 : 0.286927 @1000 / best score is 537.000000 @ 486\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.264232\tvalid_1's gini: 0.250873\n",
      "[200]\ttraining's gini: 0.29724\tvalid_1's gini: 0.269681\n",
      "[300]\ttraining's gini: 0.317898\tvalid_1's gini: 0.277219\n",
      "[400]\ttraining's gini: 0.332754\tvalid_1's gini: 0.280248\n",
      "[500]\ttraining's gini: 0.344772\tvalid_1's gini: 0.281691\n",
      "[600]\ttraining's gini: 0.355662\tvalid_1's gini: 0.281759\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's gini: 0.351911\tvalid_1's gini: 0.281985\n",
      "Fold  2 : 0.281951 @1000 / best score is 614.000000 @ 563\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.266406\tvalid_1's gini: 0.240573\n",
      "[200]\ttraining's gini: 0.299963\tvalid_1's gini: 0.26086\n",
      "[300]\ttraining's gini: 0.321441\tvalid_1's gini: 0.270987\n",
      "[400]\ttraining's gini: 0.336078\tvalid_1's gini: 0.273939\n",
      "[500]\ttraining's gini: 0.348413\tvalid_1's gini: 0.274753\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's gini: 0.349507\tvalid_1's gini: 0.274882\n",
      "Fold  3 : 0.274810 @1000 / best score is 560.000000 @ 509\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.261136\tvalid_1's gini: 0.267507\n",
      "[200]\ttraining's gini: 0.293131\tvalid_1's gini: 0.286768\n",
      "[300]\ttraining's gini: 0.314533\tvalid_1's gini: 0.294499\n",
      "[400]\ttraining's gini: 0.329849\tvalid_1's gini: 0.297212\n",
      "[500]\ttraining's gini: 0.34219\tvalid_1's gini: 0.29806\n",
      "[600]\ttraining's gini: 0.353389\tvalid_1's gini: 0.298646\n",
      "[700]\ttraining's gini: 0.364018\tvalid_1's gini: 0.299109\n",
      "Early stopping, best iteration is:\n",
      "[738]\ttraining's gini: 0.367551\tvalid_1's gini: 0.299184\n",
      "Fold  4 : 0.299152 @1000 / best score is 788.000000 @ 737\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.266115\tvalid_1's gini: 0.245621\n",
      "[200]\ttraining's gini: 0.29698\tvalid_1's gini: 0.267595\n",
      "[300]\ttraining's gini: 0.316982\tvalid_1's gini: 0.277543\n",
      "[400]\ttraining's gini: 0.331757\tvalid_1's gini: 0.282929\n",
      "[500]\ttraining's gini: 0.343632\tvalid_1's gini: 0.284799\n",
      "[600]\ttraining's gini: 0.354637\tvalid_1's gini: 0.286068\n",
      "[700]\ttraining's gini: 0.364554\tvalid_1's gini: 0.286909\n",
      "Early stopping, best iteration is:\n",
      "[703]\ttraining's gini: 0.364858\tvalid_1's gini: 0.286983\n",
      "Fold  5 : 0.286965 @1000 / best score is 753.000000 @ 702\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a7021daaa5ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m              best_round))\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Full OOF score : %.6f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgini_normalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y, y)):\n",
    "    trn_dat, trn_tgt = train.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_dat, val_tgt = train.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    clf = lgb.LGBMModel(n_estimators=n_estimators,\n",
    "                        num_leaves=20,\n",
    "   #                     boosting_type = 'dart',\n",
    "                        objective=\"binary\",\n",
    "                        learning_rate=.025, \n",
    "                        subsample=.9, \n",
    "                        colsample_bytree=.7,\n",
    "         #               min_split_gain = .77,\n",
    "        #                min_child_samples= 500,\n",
    "         #               is_unbalance = True,\n",
    "                        min_child_weight =100,\n",
    "                        reg_alpha=4,\n",
    "          #              reg_lambda=2,\n",
    "                      #  nthread=2\n",
    "                       )\n",
    "    # Upsample during cross validation to avoid having the same samples\n",
    "    # in both train and validation sets\n",
    "    # Validation set is not up-sampled to monitor overfitting\n",
    "    if increase:\n",
    "        # Get positive examples\n",
    "        pos = pd.Series(trn_tgt == 1)\n",
    "        # Add positive examples\n",
    "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "        # Shuffle data\n",
    "        idx = np.arange(len(trn_dat))\n",
    "        np.random.shuffle(idx)\n",
    "        trn_dat = trn_dat.iloc[idx]\n",
    "        trn_tgt = trn_tgt.iloc[idx]\n",
    "        \n",
    "    clf.fit(trn_dat, trn_tgt, \n",
    "            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n",
    "            categorical_feature = f_cats,\n",
    "            eval_metric=gini_lgb,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "    # Find best round for validation setA\n",
    "    lgb_evals = clf.evals_result_[\"valid_1\"][\"gini\"]\n",
    "    #print(clf.evals_result_ )\n",
    "    # Keep feature importances\n",
    "    imp_df[:, fold_] = clf.feature_importances_\n",
    "    #Xgboost provides best round starting from 0 so it has to be incremented\n",
    "    best_round = np.argsort(lgb_evals)[::-1][0]\n",
    "\n",
    "    # Predict OOF and submission probas with the best round\n",
    "    oof[val_idx] = clf.predict(val_dat, num_iteration=best_round)\n",
    "    # Update submission\n",
    "    sub_preds[:, fold_] = clf.predict(test, num_iteration=best_round)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n",
    "          % (fold_ + 1,\n",
    "             gini_normalized(val_tgt, oof[val_idx]),\n",
    "             n_estimators,\n",
    "             len(lgb_evals),\n",
    "             best_round))\n",
    "          \n",
    "print(\"Full OOF score : %.6f\" % gini_normalized(y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02943459,  0.0249817 ,  0.02366248, ...,  0.03725916,\n",
       "        0.02179056,  0.03386809])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full OOF score : 0.285804\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = testid\n",
    "sub[\"target\"] =scipy.stats.hmean(sub_preds,axis=1)\n",
    "sub.to_csv(\"submission_10312017_LGB_.285805_5kfold.csv\", index=False, float_format=\"%.9f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can generally do is,\n",
    "\n",
    "Create a train-test-split as well as the validation split, use the train data to create the algorithm, optimize it on the validation set and then use the test data to get the solution\n",
    "\n",
    "but the best way is,\n",
    "\n",
    "Use K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again one problem, since there is a large number of 0s and very few number of 1s in the result, the model might favour the minority class and not the majority class. Using stratified sampling, you can use the sample proportion of 0s and 1s in every validation, this would help in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Dependence Plots\n",
    "\n",
    "Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the ‘complement’ features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deployment level code, we should use modularization. Everything should be written as function or as class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
