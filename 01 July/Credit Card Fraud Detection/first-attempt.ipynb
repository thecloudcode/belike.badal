{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-17T06:46:26.726211Z","iopub.status.busy":"2024-03-17T06:46:26.725811Z","iopub.status.idle":"2024-03-17T06:46:26.732268Z","shell.execute_reply":"2024-03-17T06:46:26.731161Z","shell.execute_reply.started":"2024-03-17T06:46:26.726180Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# import tensorflow as tf\n","pd.set_option('display.max_columns', None)\n","\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:26.735706Z","iopub.status.busy":"2024-03-17T06:46:26.735040Z","iopub.status.idle":"2024-03-17T06:46:28.311784Z","shell.execute_reply":"2024-03-17T06:46:28.310562Z","shell.execute_reply.started":"2024-03-17T06:46:26.735674Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","s = pd.read_csv(\"sample_submission.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.314916Z","iopub.status.busy":"2024-03-17T06:46:28.314453Z","iopub.status.idle":"2024-03-17T06:46:28.359369Z","shell.execute_reply":"2024-03-17T06:46:28.358069Z","shell.execute_reply.started":"2024-03-17T06:46:28.314875Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Time</th>\n","      <th>feat1</th>\n","      <th>feat2</th>\n","      <th>feat3</th>\n","      <th>feat4</th>\n","      <th>feat5</th>\n","      <th>feat6</th>\n","      <th>feat7</th>\n","      <th>feat8</th>\n","      <th>feat9</th>\n","      <th>feat10</th>\n","      <th>feat11</th>\n","      <th>feat12</th>\n","      <th>feat13</th>\n","      <th>feat14</th>\n","      <th>feat15</th>\n","      <th>feat16</th>\n","      <th>feat17</th>\n","      <th>feat18</th>\n","      <th>feat19</th>\n","      <th>feat20</th>\n","      <th>feat21</th>\n","      <th>feat22</th>\n","      <th>feat23</th>\n","      <th>feat24</th>\n","      <th>feat25</th>\n","      <th>feat26</th>\n","      <th>feat27</th>\n","      <th>feat28</th>\n","      <th>Transaction_Amount</th>\n","      <th>IsFraud</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>2.074329</td>\n","      <td>-0.129425</td>\n","      <td>-1.137418</td>\n","      <td>0.412846</td>\n","      <td>-0.192638</td>\n","      <td>-1.210144</td>\n","      <td>0.110697</td>\n","      <td>-0.263477</td>\n","      <td>0.742144</td>\n","      <td>0.108782</td>\n","      <td>-1.070243</td>\n","      <td>-0.234910</td>\n","      <td>-1.099360</td>\n","      <td>0.502467</td>\n","      <td>0.169318</td>\n","      <td>0.065688</td>\n","      <td>-0.306957</td>\n","      <td>-0.323800</td>\n","      <td>0.103348</td>\n","      <td>-0.292969</td>\n","      <td>-0.334701</td>\n","      <td>-0.887840</td>\n","      <td>0.336701</td>\n","      <td>-0.110835</td>\n","      <td>-0.291459</td>\n","      <td>0.207733</td>\n","      <td>-0.076576</td>\n","      <td>-0.059577</td>\n","      <td>1.98</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.998827</td>\n","      <td>-1.250891</td>\n","      <td>-0.520969</td>\n","      <td>-0.894539</td>\n","      <td>-1.122528</td>\n","      <td>-0.270866</td>\n","      <td>-1.029289</td>\n","      <td>0.050198</td>\n","      <td>-0.109948</td>\n","      <td>0.908773</td>\n","      <td>0.836798</td>\n","      <td>-0.056580</td>\n","      <td>-0.120990</td>\n","      <td>-0.144028</td>\n","      <td>-0.039582</td>\n","      <td>1.653057</td>\n","      <td>-0.253599</td>\n","      <td>-0.814354</td>\n","      <td>0.716784</td>\n","      <td>0.065717</td>\n","      <td>0.054848</td>\n","      <td>-0.038367</td>\n","      <td>0.133518</td>\n","      <td>-0.461928</td>\n","      <td>-0.465491</td>\n","      <td>-0.464655</td>\n","      <td>-0.009413</td>\n","      <td>-0.038238</td>\n","      <td>84.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.091535</td>\n","      <td>1.004517</td>\n","      <td>-0.223445</td>\n","      <td>-0.435249</td>\n","      <td>0.667548</td>\n","      <td>-0.988351</td>\n","      <td>0.948146</td>\n","      <td>-0.084789</td>\n","      <td>-0.042027</td>\n","      <td>-0.818383</td>\n","      <td>-0.376512</td>\n","      <td>-0.226546</td>\n","      <td>-0.552869</td>\n","      <td>-0.886466</td>\n","      <td>-0.180890</td>\n","      <td>0.230286</td>\n","      <td>0.590579</td>\n","      <td>-0.321590</td>\n","      <td>-0.433959</td>\n","      <td>-0.021375</td>\n","      <td>-0.326725</td>\n","      <td>-0.803736</td>\n","      <td>0.154495</td>\n","      <td>0.951233</td>\n","      <td>-0.506919</td>\n","      <td>0.085046</td>\n","      <td>0.224458</td>\n","      <td>0.087356</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>1.979649</td>\n","      <td>-0.184949</td>\n","      <td>-1.064206</td>\n","      <td>0.120125</td>\n","      <td>-0.215238</td>\n","      <td>-0.648829</td>\n","      <td>-0.087826</td>\n","      <td>-0.035367</td>\n","      <td>0.885838</td>\n","      <td>-0.007527</td>\n","      <td>0.637441</td>\n","      <td>0.676960</td>\n","      <td>-1.504823</td>\n","      <td>0.554039</td>\n","      <td>-0.824356</td>\n","      <td>-0.527267</td>\n","      <td>-0.095838</td>\n","      <td>-0.312519</td>\n","      <td>0.642659</td>\n","      <td>-0.340089</td>\n","      <td>-0.095514</td>\n","      <td>-0.079792</td>\n","      <td>0.167701</td>\n","      <td>-0.042939</td>\n","      <td>0.000799</td>\n","      <td>-0.096148</td>\n","      <td>-0.057780</td>\n","      <td>-0.073839</td>\n","      <td>1.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>1.025898</td>\n","      <td>-0.171827</td>\n","      <td>1.203717</td>\n","      <td>1.243900</td>\n","      <td>-0.636572</td>\n","      <td>1.099074</td>\n","      <td>-0.938651</td>\n","      <td>0.569239</td>\n","      <td>0.692665</td>\n","      <td>-0.097495</td>\n","      <td>1.338869</td>\n","      <td>1.391399</td>\n","      <td>-0.128167</td>\n","      <td>-0.081836</td>\n","      <td>0.100548</td>\n","      <td>-0.338937</td>\n","      <td>0.090864</td>\n","      <td>-0.423645</td>\n","      <td>-0.731939</td>\n","      <td>-0.203628</td>\n","      <td>0.099157</td>\n","      <td>0.608908</td>\n","      <td>0.027901</td>\n","      <td>-0.262813</td>\n","      <td>0.257834</td>\n","      <td>-0.252829</td>\n","      <td>0.108338</td>\n","      <td>0.021051</td>\n","      <td>1.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>149995</th>\n","      <td>149995</td>\n","      <td>73899.0</td>\n","      <td>1.277125</td>\n","      <td>0.665683</td>\n","      <td>-0.688148</td>\n","      <td>1.135626</td>\n","      <td>0.494826</td>\n","      <td>-0.554938</td>\n","      <td>0.252478</td>\n","      <td>-0.132739</td>\n","      <td>0.011839</td>\n","      <td>-0.569079</td>\n","      <td>-0.723579</td>\n","      <td>-0.488348</td>\n","      <td>-0.102659</td>\n","      <td>-1.301052</td>\n","      <td>1.221338</td>\n","      <td>0.608728</td>\n","      <td>0.460123</td>\n","      <td>0.766129</td>\n","      <td>-0.317815</td>\n","      <td>-0.077668</td>\n","      <td>-0.114747</td>\n","      <td>-0.221548</td>\n","      <td>-0.233038</td>\n","      <td>-0.744995</td>\n","      <td>0.799359</td>\n","      <td>-0.244856</td>\n","      <td>0.037868</td>\n","      <td>0.042670</td>\n","      <td>1.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149996</th>\n","      <td>149996</td>\n","      <td>73899.0</td>\n","      <td>0.807735</td>\n","      <td>-1.813163</td>\n","      <td>0.421073</td>\n","      <td>-0.576839</td>\n","      <td>-1.601656</td>\n","      <td>-0.665517</td>\n","      <td>-0.230382</td>\n","      <td>-0.297095</td>\n","      <td>-0.756538</td>\n","      <td>0.622136</td>\n","      <td>-0.120869</td>\n","      <td>0.042765</td>\n","      <td>1.441500</td>\n","      <td>-0.532401</td>\n","      <td>0.247528</td>\n","      <td>0.978543</td>\n","      <td>0.365566</td>\n","      <td>-1.703377</td>\n","      <td>0.490932</td>\n","      <td>0.653768</td>\n","      <td>0.270879</td>\n","      <td>0.014436</td>\n","      <td>-0.193669</td>\n","      <td>0.387714</td>\n","      <td>0.169280</td>\n","      <td>-0.444501</td>\n","      <td>-0.043788</td>\n","      <td>0.072515</td>\n","      <td>349.70</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149997</th>\n","      <td>149997</td>\n","      <td>73899.0</td>\n","      <td>0.836403</td>\n","      <td>-0.351598</td>\n","      <td>0.650338</td>\n","      <td>1.066155</td>\n","      <td>-0.234826</td>\n","      <td>0.844271</td>\n","      <td>-0.409578</td>\n","      <td>0.382619</td>\n","      <td>0.066264</td>\n","      <td>-0.001598</td>\n","      <td>1.826985</td>\n","      <td>0.976253</td>\n","      <td>-0.253430</td>\n","      <td>0.485950</td>\n","      <td>1.186825</td>\n","      <td>-0.134916</td>\n","      <td>-0.112044</td>\n","      <td>-0.399080</td>\n","      <td>-1.098147</td>\n","      <td>0.009924</td>\n","      <td>0.285914</td>\n","      <td>0.721022</td>\n","      <td>-0.067894</td>\n","      <td>-0.273675</td>\n","      <td>0.232969</td>\n","      <td>-0.286735</td>\n","      <td>0.054066</td>\n","      <td>0.031396</td>\n","      <td>116.10</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149998</th>\n","      <td>149998</td>\n","      <td>73899.0</td>\n","      <td>-0.806965</td>\n","      <td>0.383847</td>\n","      <td>2.296469</td>\n","      <td>1.428714</td>\n","      <td>-2.343948</td>\n","      <td>1.073324</td>\n","      <td>-0.203567</td>\n","      <td>0.456589</td>\n","      <td>1.093465</td>\n","      <td>-0.217076</td>\n","      <td>-0.999571</td>\n","      <td>0.065602</td>\n","      <td>-0.557980</td>\n","      <td>-0.923127</td>\n","      <td>-0.232334</td>\n","      <td>-1.002802</td>\n","      <td>1.063369</td>\n","      <td>0.222880</td>\n","      <td>2.333524</td>\n","      <td>0.184824</td>\n","      <td>0.228740</td>\n","      <td>1.296798</td>\n","      <td>-0.038753</td>\n","      <td>0.827484</td>\n","      <td>-0.743000</td>\n","      <td>0.914488</td>\n","      <td>0.307576</td>\n","      <td>-0.010200</td>\n","      <td>179.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149999</th>\n","      <td>149999</td>\n","      <td>73899.0</td>\n","      <td>1.146412</td>\n","      <td>-1.058955</td>\n","      <td>0.726353</td>\n","      <td>0.425955</td>\n","      <td>-1.150291</td>\n","      <td>0.009909</td>\n","      <td>-0.856315</td>\n","      <td>0.025132</td>\n","      <td>-1.052905</td>\n","      <td>0.717166</td>\n","      <td>-1.581179</td>\n","      <td>-0.947241</td>\n","      <td>0.550133</td>\n","      <td>-0.158677</td>\n","      <td>1.097885</td>\n","      <td>-0.724134</td>\n","      <td>-0.596835</td>\n","      <td>1.854181</td>\n","      <td>-1.000315</td>\n","      <td>-0.361314</td>\n","      <td>-0.308839</td>\n","      <td>-0.634263</td>\n","      <td>-0.121748</td>\n","      <td>-0.566361</td>\n","      <td>0.353399</td>\n","      <td>-0.302464</td>\n","      <td>0.064652</td>\n","      <td>0.037291</td>\n","      <td>118.00</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150000 rows × 32 columns</p>\n","</div>"],"text/plain":["            id     Time     feat1     feat2     feat3     feat4     feat5  \\\n","0            0      0.0  2.074329 -0.129425 -1.137418  0.412846 -0.192638   \n","1            1      0.0  1.998827 -1.250891 -0.520969 -0.894539 -1.122528   \n","2            2      0.0  0.091535  1.004517 -0.223445 -0.435249  0.667548   \n","3            3      0.0  1.979649 -0.184949 -1.064206  0.120125 -0.215238   \n","4            4      0.0  1.025898 -0.171827  1.203717  1.243900 -0.636572   \n","...        ...      ...       ...       ...       ...       ...       ...   \n","149995  149995  73899.0  1.277125  0.665683 -0.688148  1.135626  0.494826   \n","149996  149996  73899.0  0.807735 -1.813163  0.421073 -0.576839 -1.601656   \n","149997  149997  73899.0  0.836403 -0.351598  0.650338  1.066155 -0.234826   \n","149998  149998  73899.0 -0.806965  0.383847  2.296469  1.428714 -2.343948   \n","149999  149999  73899.0  1.146412 -1.058955  0.726353  0.425955 -1.150291   \n","\n","           feat6     feat7     feat8     feat9    feat10    feat11    feat12  \\\n","0      -1.210144  0.110697 -0.263477  0.742144  0.108782 -1.070243 -0.234910   \n","1      -0.270866 -1.029289  0.050198 -0.109948  0.908773  0.836798 -0.056580   \n","2      -0.988351  0.948146 -0.084789 -0.042027 -0.818383 -0.376512 -0.226546   \n","3      -0.648829 -0.087826 -0.035367  0.885838 -0.007527  0.637441  0.676960   \n","4       1.099074 -0.938651  0.569239  0.692665 -0.097495  1.338869  1.391399   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","149995 -0.554938  0.252478 -0.132739  0.011839 -0.569079 -0.723579 -0.488348   \n","149996 -0.665517 -0.230382 -0.297095 -0.756538  0.622136 -0.120869  0.042765   \n","149997  0.844271 -0.409578  0.382619  0.066264 -0.001598  1.826985  0.976253   \n","149998  1.073324 -0.203567  0.456589  1.093465 -0.217076 -0.999571  0.065602   \n","149999  0.009909 -0.856315  0.025132 -1.052905  0.717166 -1.581179 -0.947241   \n","\n","          feat13    feat14    feat15    feat16    feat17    feat18    feat19  \\\n","0      -1.099360  0.502467  0.169318  0.065688 -0.306957 -0.323800  0.103348   \n","1      -0.120990 -0.144028 -0.039582  1.653057 -0.253599 -0.814354  0.716784   \n","2      -0.552869 -0.886466 -0.180890  0.230286  0.590579 -0.321590 -0.433959   \n","3      -1.504823  0.554039 -0.824356 -0.527267 -0.095838 -0.312519  0.642659   \n","4      -0.128167 -0.081836  0.100548 -0.338937  0.090864 -0.423645 -0.731939   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","149995 -0.102659 -1.301052  1.221338  0.608728  0.460123  0.766129 -0.317815   \n","149996  1.441500 -0.532401  0.247528  0.978543  0.365566 -1.703377  0.490932   \n","149997 -0.253430  0.485950  1.186825 -0.134916 -0.112044 -0.399080 -1.098147   \n","149998 -0.557980 -0.923127 -0.232334 -1.002802  1.063369  0.222880  2.333524   \n","149999  0.550133 -0.158677  1.097885 -0.724134 -0.596835  1.854181 -1.000315   \n","\n","          feat20    feat21    feat22    feat23    feat24    feat25    feat26  \\\n","0      -0.292969 -0.334701 -0.887840  0.336701 -0.110835 -0.291459  0.207733   \n","1       0.065717  0.054848 -0.038367  0.133518 -0.461928 -0.465491 -0.464655   \n","2      -0.021375 -0.326725 -0.803736  0.154495  0.951233 -0.506919  0.085046   \n","3      -0.340089 -0.095514 -0.079792  0.167701 -0.042939  0.000799 -0.096148   \n","4      -0.203628  0.099157  0.608908  0.027901 -0.262813  0.257834 -0.252829   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","149995 -0.077668 -0.114747 -0.221548 -0.233038 -0.744995  0.799359 -0.244856   \n","149996  0.653768  0.270879  0.014436 -0.193669  0.387714  0.169280 -0.444501   \n","149997  0.009924  0.285914  0.721022 -0.067894 -0.273675  0.232969 -0.286735   \n","149998  0.184824  0.228740  1.296798 -0.038753  0.827484 -0.743000  0.914488   \n","149999 -0.361314 -0.308839 -0.634263 -0.121748 -0.566361  0.353399 -0.302464   \n","\n","          feat27    feat28  Transaction_Amount  IsFraud  \n","0      -0.076576 -0.059577                1.98        0  \n","1      -0.009413 -0.038238               84.00        0  \n","2       0.224458  0.087356                2.69        0  \n","3      -0.057780 -0.073839                1.00        0  \n","4       0.108338  0.021051                1.00        0  \n","...          ...       ...                 ...      ...  \n","149995  0.037868  0.042670                1.00        0  \n","149996 -0.043788  0.072515              349.70        0  \n","149997  0.054066  0.031396              116.10        0  \n","149998  0.307576 -0.010200              179.00        0  \n","149999  0.064652  0.037291              118.00        0  \n","\n","[150000 rows x 32 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.361304Z","iopub.status.busy":"2024-03-17T06:46:28.360893Z","iopub.status.idle":"2024-03-17T06:46:28.382201Z","shell.execute_reply":"2024-03-17T06:46:28.381094Z","shell.execute_reply.started":"2024-03-17T06:46:28.361266Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150000 entries, 0 to 149999\n","Data columns (total 32 columns):\n"," #   Column              Non-Null Count   Dtype  \n","---  ------              --------------   -----  \n"," 0   id                  150000 non-null  int64  \n"," 1   Time                150000 non-null  float64\n"," 2   feat1               150000 non-null  float64\n"," 3   feat2               150000 non-null  float64\n"," 4   feat3               150000 non-null  float64\n"," 5   feat4               150000 non-null  float64\n"," 6   feat5               150000 non-null  float64\n"," 7   feat6               150000 non-null  float64\n"," 8   feat7               150000 non-null  float64\n"," 9   feat8               150000 non-null  float64\n"," 10  feat9               150000 non-null  float64\n"," 11  feat10              150000 non-null  float64\n"," 12  feat11              150000 non-null  float64\n"," 13  feat12              150000 non-null  float64\n"," 14  feat13              150000 non-null  float64\n"," 15  feat14              150000 non-null  float64\n"," 16  feat15              150000 non-null  float64\n"," 17  feat16              150000 non-null  float64\n"," 18  feat17              150000 non-null  float64\n"," 19  feat18              150000 non-null  float64\n"," 20  feat19              150000 non-null  float64\n"," 21  feat20              150000 non-null  float64\n"," 22  feat21              150000 non-null  float64\n"," 23  feat22              150000 non-null  float64\n"," 24  feat23              150000 non-null  float64\n"," 25  feat24              150000 non-null  float64\n"," 26  feat25              150000 non-null  float64\n"," 27  feat26              150000 non-null  float64\n"," 28  feat27              150000 non-null  float64\n"," 29  feat28              150000 non-null  float64\n"," 30  Transaction_Amount  150000 non-null  float64\n"," 31  IsFraud             150000 non-null  int64  \n","dtypes: float64(30), int64(2)\n","memory usage: 36.6 MB\n"]}],"source":["data.info()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.386034Z","iopub.status.busy":"2024-03-17T06:46:28.385252Z","iopub.status.idle":"2024-03-17T06:46:28.396680Z","shell.execute_reply":"2024-03-17T06:46:28.395370Z","shell.execute_reply.started":"2024-03-17T06:46:28.385982Z"},"trusted":true},"outputs":[{"data":{"text/plain":["IsFraud\n","0    149731\n","1       269\n","Name: count, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data['IsFraud'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### Preprccessing"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.398450Z","iopub.status.busy":"2024-03-17T06:46:28.398102Z","iopub.status.idle":"2024-03-17T06:46:28.611636Z","shell.execute_reply":"2024-03-17T06:46:28.610414Z","shell.execute_reply.started":"2024-03-17T06:46:28.398413Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'id': 150000,\n"," 'Time': 20745,\n"," 'feat1': 149170,\n"," 'feat2': 149201,\n"," 'feat3': 149160,\n"," 'feat4': 149170,\n"," 'feat5': 149167,\n"," 'feat6': 149066,\n"," 'feat7': 149202,\n"," 'feat8': 149135,\n"," 'feat9': 149149,\n"," 'feat10': 149171,\n"," 'feat11': 149130,\n"," 'feat12': 149177,\n"," 'feat13': 149201,\n"," 'feat14': 149142,\n"," 'feat15': 149194,\n"," 'feat16': 149194,\n"," 'feat17': 149175,\n"," 'feat18': 149180,\n"," 'feat19': 149192,\n"," 'feat20': 149160,\n"," 'feat21': 149138,\n"," 'feat22': 149211,\n"," 'feat23': 149161,\n"," 'feat24': 149112,\n"," 'feat25': 149193,\n"," 'feat26': 149192,\n"," 'feat27': 149069,\n"," 'feat28': 148970,\n"," 'Transaction_Amount': 16731,\n"," 'IsFraud': 2}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["{column: len(data[column].unique()) for column in data.columns}"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.613878Z","iopub.status.busy":"2024-03-17T06:46:28.613449Z","iopub.status.idle":"2024-03-17T06:46:28.622391Z","shell.execute_reply":"2024-03-17T06:46:28.621053Z","shell.execute_reply.started":"2024-03-17T06:46:28.613840Z"},"trusted":true},"outputs":[],"source":["def preprocess_inputs(df):\n","    df = df.copy()\n","    \n","    df = df.drop(['id'], axis=1)\n","    \n","    y = df['IsFraud'].copy()\n","    X = df.drop('IsFraud', axis = 1).copy()\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 123)\n","    \n","    scaler = StandardScaler()\n","    scaler.fit(X_train)\n","    \n","    X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n","    X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n","    \n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.624470Z","iopub.status.busy":"2024-03-17T06:46:28.624066Z","iopub.status.idle":"2024-03-17T06:46:28.800011Z","shell.execute_reply":"2024-03-17T06:46:28.798898Z","shell.execute_reply.started":"2024-03-17T06:46:28.624434Z"},"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = preprocess_inputs(data)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.801869Z","iopub.status.busy":"2024-03-17T06:46:28.801498Z","iopub.status.idle":"2024-03-17T06:46:28.841788Z","shell.execute_reply":"2024-03-17T06:46:28.840709Z","shell.execute_reply.started":"2024-03-17T06:46:28.801840Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>feat1</th>\n","      <th>feat2</th>\n","      <th>feat3</th>\n","      <th>feat4</th>\n","      <th>feat5</th>\n","      <th>feat6</th>\n","      <th>feat7</th>\n","      <th>feat8</th>\n","      <th>feat9</th>\n","      <th>feat10</th>\n","      <th>feat11</th>\n","      <th>feat12</th>\n","      <th>feat13</th>\n","      <th>feat14</th>\n","      <th>feat15</th>\n","      <th>feat16</th>\n","      <th>feat17</th>\n","      <th>feat18</th>\n","      <th>feat19</th>\n","      <th>feat20</th>\n","      <th>feat21</th>\n","      <th>feat22</th>\n","      <th>feat23</th>\n","      <th>feat24</th>\n","      <th>feat25</th>\n","      <th>feat26</th>\n","      <th>feat27</th>\n","      <th>feat28</th>\n","      <th>Transaction_Amount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.605987</td>\n","      <td>-0.579936</td>\n","      <td>1.350908</td>\n","      <td>1.083101</td>\n","      <td>1.991932</td>\n","      <td>0.712025</td>\n","      <td>0.167394</td>\n","      <td>1.039911</td>\n","      <td>-0.104928</td>\n","      <td>-2.160363</td>\n","      <td>0.921554</td>\n","      <td>-0.427283</td>\n","      <td>0.423795</td>\n","      <td>2.096704</td>\n","      <td>0.003399</td>\n","      <td>1.165734</td>\n","      <td>-0.034925</td>\n","      <td>-0.199062</td>\n","      <td>-0.649386</td>\n","      <td>-0.116900</td>\n","      <td>-0.039909</td>\n","      <td>0.080851</td>\n","      <td>0.214949</td>\n","      <td>-0.190066</td>\n","      <td>0.225420</td>\n","      <td>-0.596441</td>\n","      <td>0.310375</td>\n","      <td>-0.241014</td>\n","      <td>0.458903</td>\n","      <td>-0.351543</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.567978</td>\n","      <td>-2.201131</td>\n","      <td>-0.614922</td>\n","      <td>-0.801349</td>\n","      <td>-2.384910</td>\n","      <td>0.760731</td>\n","      <td>-1.632905</td>\n","      <td>0.302920</td>\n","      <td>0.221511</td>\n","      <td>-0.471476</td>\n","      <td>-0.771419</td>\n","      <td>1.084920</td>\n","      <td>0.631769</td>\n","      <td>-1.710772</td>\n","      <td>1.539223</td>\n","      <td>-0.743246</td>\n","      <td>-0.374833</td>\n","      <td>-1.407724</td>\n","      <td>1.576159</td>\n","      <td>-1.418647</td>\n","      <td>-1.409437</td>\n","      <td>-0.251637</td>\n","      <td>0.275095</td>\n","      <td>2.448019</td>\n","      <td>0.801495</td>\n","      <td>0.588341</td>\n","      <td>-1.772040</td>\n","      <td>1.614959</td>\n","      <td>-1.105157</td>\n","      <td>-0.418067</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.750881</td>\n","      <td>1.498420</td>\n","      <td>-0.005955</td>\n","      <td>-2.605520</td>\n","      <td>0.195694</td>\n","      <td>0.694679</td>\n","      <td>-0.512877</td>\n","      <td>0.191055</td>\n","      <td>-0.278650</td>\n","      <td>0.962479</td>\n","      <td>-0.384375</td>\n","      <td>-1.315325</td>\n","      <td>-0.888294</td>\n","      <td>-1.525370</td>\n","      <td>-0.679713</td>\n","      <td>0.327865</td>\n","      <td>0.539402</td>\n","      <td>0.640949</td>\n","      <td>0.027017</td>\n","      <td>0.139889</td>\n","      <td>-0.573493</td>\n","      <td>-0.873278</td>\n","      <td>-1.793136</td>\n","      <td>1.145714</td>\n","      <td>-0.215344</td>\n","      <td>-1.224568</td>\n","      <td>0.400691</td>\n","      <td>-0.390125</td>\n","      <td>-0.356242</td>\n","      <td>-0.424035</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.004469</td>\n","      <td>-1.149714</td>\n","      <td>0.952337</td>\n","      <td>0.074564</td>\n","      <td>-0.944146</td>\n","      <td>-0.593292</td>\n","      <td>-1.099060</td>\n","      <td>0.108553</td>\n","      <td>0.985424</td>\n","      <td>0.257351</td>\n","      <td>-1.067320</td>\n","      <td>-0.316141</td>\n","      <td>-0.123461</td>\n","      <td>-1.198122</td>\n","      <td>0.821613</td>\n","      <td>0.700608</td>\n","      <td>0.266881</td>\n","      <td>0.316385</td>\n","      <td>-0.966408</td>\n","      <td>-1.514584</td>\n","      <td>-0.373034</td>\n","      <td>-0.167440</td>\n","      <td>-0.390030</td>\n","      <td>0.308194</td>\n","      <td>0.971600</td>\n","      <td>-0.720582</td>\n","      <td>1.720384</td>\n","      <td>0.413393</td>\n","      <td>-0.184562</td>\n","      <td>-0.440304</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.236971</td>\n","      <td>-1.600098</td>\n","      <td>1.377071</td>\n","      <td>-1.841893</td>\n","      <td>-1.524467</td>\n","      <td>2.226337</td>\n","      <td>2.639262</td>\n","      <td>0.511164</td>\n","      <td>1.320116</td>\n","      <td>0.391062</td>\n","      <td>1.137804</td>\n","      <td>-0.309272</td>\n","      <td>0.187227</td>\n","      <td>0.747987</td>\n","      <td>0.098155</td>\n","      <td>1.059413</td>\n","      <td>0.337805</td>\n","      <td>-1.205560</td>\n","      <td>0.366562</td>\n","      <td>0.083761</td>\n","      <td>1.189263</td>\n","      <td>0.286585</td>\n","      <td>0.137673</td>\n","      <td>0.588780</td>\n","      <td>1.756498</td>\n","      <td>0.028408</td>\n","      <td>0.634882</td>\n","      <td>1.645391</td>\n","      <td>3.001880</td>\n","      <td>-0.302106</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>104995</th>\n","      <td>1.242571</td>\n","      <td>-0.724395</td>\n","      <td>1.261111</td>\n","      <td>1.278857</td>\n","      <td>2.322340</td>\n","      <td>-0.490254</td>\n","      <td>0.139270</td>\n","      <td>-0.165526</td>\n","      <td>0.890162</td>\n","      <td>-0.455232</td>\n","      <td>0.242864</td>\n","      <td>-1.276659</td>\n","      <td>0.256462</td>\n","      <td>0.246810</td>\n","      <td>0.267005</td>\n","      <td>0.038862</td>\n","      <td>-0.074259</td>\n","      <td>0.564980</td>\n","      <td>0.279900</td>\n","      <td>1.165863</td>\n","      <td>-0.023549</td>\n","      <td>0.468836</td>\n","      <td>1.054057</td>\n","      <td>0.161023</td>\n","      <td>0.716466</td>\n","      <td>-1.163623</td>\n","      <td>0.552891</td>\n","      <td>0.221145</td>\n","      <td>0.399937</td>\n","      <td>-0.422715</td>\n","    </tr>\n","    <tr>\n","      <th>104996</th>\n","      <td>0.869350</td>\n","      <td>-0.716189</td>\n","      <td>1.077431</td>\n","      <td>0.442280</td>\n","      <td>-0.086999</td>\n","      <td>-0.390156</td>\n","      <td>-0.553594</td>\n","      <td>0.650130</td>\n","      <td>0.243709</td>\n","      <td>-0.624874</td>\n","      <td>-0.063015</td>\n","      <td>1.126515</td>\n","      <td>1.147006</td>\n","      <td>0.230889</td>\n","      <td>0.644338</td>\n","      <td>-0.341495</td>\n","      <td>-0.137883</td>\n","      <td>-0.068880</td>\n","      <td>0.118798</td>\n","      <td>1.439197</td>\n","      <td>-0.042313</td>\n","      <td>-0.012094</td>\n","      <td>-0.127311</td>\n","      <td>0.416907</td>\n","      <td>0.970739</td>\n","      <td>-2.341691</td>\n","      <td>0.275980</td>\n","      <td>-0.589615</td>\n","      <td>0.577933</td>\n","      <td>-0.132561</td>\n","    </tr>\n","    <tr>\n","      <th>104997</th>\n","      <td>-1.196623</td>\n","      <td>0.909011</td>\n","      <td>0.293415</td>\n","      <td>-0.898783</td>\n","      <td>0.155704</td>\n","      <td>0.534782</td>\n","      <td>-0.194419</td>\n","      <td>0.301714</td>\n","      <td>-0.282338</td>\n","      <td>-0.109636</td>\n","      <td>-0.122705</td>\n","      <td>0.566040</td>\n","      <td>0.902178</td>\n","      <td>0.880222</td>\n","      <td>-0.423685</td>\n","      <td>0.268529</td>\n","      <td>1.224100</td>\n","      <td>-1.112371</td>\n","      <td>0.713036</td>\n","      <td>0.666920</td>\n","      <td>-0.053854</td>\n","      <td>-0.658566</td>\n","      <td>-1.389845</td>\n","      <td>0.087376</td>\n","      <td>-1.443057</td>\n","      <td>0.512218</td>\n","      <td>0.265093</td>\n","      <td>-0.164843</td>\n","      <td>-0.050306</td>\n","      <td>-0.434148</td>\n","    </tr>\n","    <tr>\n","      <th>104998</th>\n","      <td>-0.753099</td>\n","      <td>0.682304</td>\n","      <td>-0.056999</td>\n","      <td>-0.018557</td>\n","      <td>0.852925</td>\n","      <td>-0.009247</td>\n","      <td>0.093821</td>\n","      <td>-0.075994</td>\n","      <td>0.042952</td>\n","      <td>0.135231</td>\n","      <td>-0.097980</td>\n","      <td>-0.570460</td>\n","      <td>0.814489</td>\n","      <td>1.386511</td>\n","      <td>-0.146068</td>\n","      <td>1.519815</td>\n","      <td>0.057927</td>\n","      <td>-0.552795</td>\n","      <td>-0.538474</td>\n","      <td>-0.781397</td>\n","      <td>0.143004</td>\n","      <td>0.183935</td>\n","      <td>0.262592</td>\n","      <td>-0.062888</td>\n","      <td>-0.465782</td>\n","      <td>0.484884</td>\n","      <td>-0.983651</td>\n","      <td>0.133288</td>\n","      <td>0.075708</td>\n","      <td>0.099928</td>\n","    </tr>\n","    <tr>\n","      <th>104999</th>\n","      <td>-1.421452</td>\n","      <td>0.790646</td>\n","      <td>0.195935</td>\n","      <td>0.065631</td>\n","      <td>1.276834</td>\n","      <td>0.000833</td>\n","      <td>-0.132501</td>\n","      <td>-0.062958</td>\n","      <td>-0.234700</td>\n","      <td>1.870792</td>\n","      <td>-0.610759</td>\n","      <td>0.520084</td>\n","      <td>-2.442675</td>\n","      <td>0.872894</td>\n","      <td>1.784634</td>\n","      <td>-0.268339</td>\n","      <td>-0.754464</td>\n","      <td>1.291921</td>\n","      <td>-0.494694</td>\n","      <td>-1.213858</td>\n","      <td>-0.651930</td>\n","      <td>-0.140300</td>\n","      <td>0.447945</td>\n","      <td>-0.079940</td>\n","      <td>0.055602</td>\n","      <td>1.193055</td>\n","      <td>-0.535467</td>\n","      <td>0.029749</td>\n","      <td>-0.053314</td>\n","      <td>-0.383769</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>105000 rows × 30 columns</p>\n","</div>"],"text/plain":["            Time     feat1     feat2     feat3     feat4     feat5     feat6  \\\n","0      -0.605987 -0.579936  1.350908  1.083101  1.991932  0.712025  0.167394   \n","1       0.567978 -2.201131 -0.614922 -0.801349 -2.384910  0.760731 -1.632905   \n","2      -1.750881  1.498420 -0.005955 -2.605520  0.195694  0.694679 -0.512877   \n","3      -0.004469 -1.149714  0.952337  0.074564 -0.944146 -0.593292 -1.099060   \n","4       1.236971 -1.600098  1.377071 -1.841893 -1.524467  2.226337  2.639262   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  1.242571 -0.724395  1.261111  1.278857  2.322340 -0.490254  0.139270   \n","104996  0.869350 -0.716189  1.077431  0.442280 -0.086999 -0.390156 -0.553594   \n","104997 -1.196623  0.909011  0.293415 -0.898783  0.155704  0.534782 -0.194419   \n","104998 -0.753099  0.682304 -0.056999 -0.018557  0.852925 -0.009247  0.093821   \n","104999 -1.421452  0.790646  0.195935  0.065631  1.276834  0.000833 -0.132501   \n","\n","           feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n","0       1.039911 -0.104928 -2.160363  0.921554 -0.427283  0.423795  2.096704   \n","1       0.302920  0.221511 -0.471476 -0.771419  1.084920  0.631769 -1.710772   \n","2       0.191055 -0.278650  0.962479 -0.384375 -1.315325 -0.888294 -1.525370   \n","3       0.108553  0.985424  0.257351 -1.067320 -0.316141 -0.123461 -1.198122   \n","4       0.511164  1.320116  0.391062  1.137804 -0.309272  0.187227  0.747987   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995 -0.165526  0.890162 -0.455232  0.242864 -1.276659  0.256462  0.246810   \n","104996  0.650130  0.243709 -0.624874 -0.063015  1.126515  1.147006  0.230889   \n","104997  0.301714 -0.282338 -0.109636 -0.122705  0.566040  0.902178  0.880222   \n","104998 -0.075994  0.042952  0.135231 -0.097980 -0.570460  0.814489  1.386511   \n","104999 -0.062958 -0.234700  1.870792 -0.610759  0.520084 -2.442675  0.872894   \n","\n","          feat14    feat15    feat16    feat17    feat18    feat19    feat20  \\\n","0       0.003399  1.165734 -0.034925 -0.199062 -0.649386 -0.116900 -0.039909   \n","1       1.539223 -0.743246 -0.374833 -1.407724  1.576159 -1.418647 -1.409437   \n","2      -0.679713  0.327865  0.539402  0.640949  0.027017  0.139889 -0.573493   \n","3       0.821613  0.700608  0.266881  0.316385 -0.966408 -1.514584 -0.373034   \n","4       0.098155  1.059413  0.337805 -1.205560  0.366562  0.083761  1.189263   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  0.267005  0.038862 -0.074259  0.564980  0.279900  1.165863 -0.023549   \n","104996  0.644338 -0.341495 -0.137883 -0.068880  0.118798  1.439197 -0.042313   \n","104997 -0.423685  0.268529  1.224100 -1.112371  0.713036  0.666920 -0.053854   \n","104998 -0.146068  1.519815  0.057927 -0.552795 -0.538474 -0.781397  0.143004   \n","104999  1.784634 -0.268339 -0.754464  1.291921 -0.494694 -1.213858 -0.651930   \n","\n","          feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n","0       0.080851  0.214949 -0.190066  0.225420 -0.596441  0.310375 -0.241014   \n","1      -0.251637  0.275095  2.448019  0.801495  0.588341 -1.772040  1.614959   \n","2      -0.873278 -1.793136  1.145714 -0.215344 -1.224568  0.400691 -0.390125   \n","3      -0.167440 -0.390030  0.308194  0.971600 -0.720582  1.720384  0.413393   \n","4       0.286585  0.137673  0.588780  1.756498  0.028408  0.634882  1.645391   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  0.468836  1.054057  0.161023  0.716466 -1.163623  0.552891  0.221145   \n","104996 -0.012094 -0.127311  0.416907  0.970739 -2.341691  0.275980 -0.589615   \n","104997 -0.658566 -1.389845  0.087376 -1.443057  0.512218  0.265093 -0.164843   \n","104998  0.183935  0.262592 -0.062888 -0.465782  0.484884 -0.983651  0.133288   \n","104999 -0.140300  0.447945 -0.079940  0.055602  1.193055 -0.535467  0.029749   \n","\n","          feat28  Transaction_Amount  \n","0       0.458903           -0.351543  \n","1      -1.105157           -0.418067  \n","2      -0.356242           -0.424035  \n","3      -0.184562           -0.440304  \n","4       3.001880           -0.302106  \n","...          ...                 ...  \n","104995  0.399937           -0.422715  \n","104996  0.577933           -0.132561  \n","104997 -0.050306           -0.434148  \n","104998  0.075708            0.099928  \n","104999 -0.053314           -0.383769  \n","\n","[105000 rows x 30 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.843602Z","iopub.status.busy":"2024-03-17T06:46:28.843207Z","iopub.status.idle":"2024-03-17T06:46:28.851757Z","shell.execute_reply":"2024-03-17T06:46:28.850567Z","shell.execute_reply.started":"2024-03-17T06:46:28.843567Z"},"trusted":true},"outputs":[{"data":{"text/plain":["33237     0\n","98431     0\n","13250     0\n","60278     0\n","145808    0\n","         ..\n","146449    0\n","119906    0\n","17730     0\n","28030     0\n","15725     0\n","Name: IsFraud, Length: 105000, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_train"]},{"cell_type":"markdown","metadata":{},"source":["### Handling Class Imbalance"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.857050Z","iopub.status.busy":"2024-03-17T06:46:28.856698Z","iopub.status.idle":"2024-03-17T06:46:28.870036Z","shell.execute_reply":"2024-03-17T06:46:28.868878Z","shell.execute_reply.started":"2024-03-17T06:46:28.857015Z"},"trusted":true},"outputs":[],"source":["train_df = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.871785Z","iopub.status.busy":"2024-03-17T06:46:28.871392Z","iopub.status.idle":"2024-03-17T06:46:28.913212Z","shell.execute_reply":"2024-03-17T06:46:28.912004Z","shell.execute_reply.started":"2024-03-17T06:46:28.871756Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>feat1</th>\n","      <th>feat2</th>\n","      <th>feat3</th>\n","      <th>feat4</th>\n","      <th>feat5</th>\n","      <th>feat6</th>\n","      <th>feat7</th>\n","      <th>feat8</th>\n","      <th>feat9</th>\n","      <th>feat10</th>\n","      <th>feat11</th>\n","      <th>feat12</th>\n","      <th>feat13</th>\n","      <th>feat14</th>\n","      <th>feat15</th>\n","      <th>feat16</th>\n","      <th>feat17</th>\n","      <th>feat18</th>\n","      <th>feat19</th>\n","      <th>feat20</th>\n","      <th>feat21</th>\n","      <th>feat22</th>\n","      <th>feat23</th>\n","      <th>feat24</th>\n","      <th>feat25</th>\n","      <th>feat26</th>\n","      <th>feat27</th>\n","      <th>feat28</th>\n","      <th>Transaction_Amount</th>\n","      <th>IsFraud</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.605987</td>\n","      <td>-0.579936</td>\n","      <td>1.350908</td>\n","      <td>1.083101</td>\n","      <td>1.991932</td>\n","      <td>0.712025</td>\n","      <td>0.167394</td>\n","      <td>1.039911</td>\n","      <td>-0.104928</td>\n","      <td>-2.160363</td>\n","      <td>0.921554</td>\n","      <td>-0.427283</td>\n","      <td>0.423795</td>\n","      <td>2.096704</td>\n","      <td>0.003399</td>\n","      <td>1.165734</td>\n","      <td>-0.034925</td>\n","      <td>-0.199062</td>\n","      <td>-0.649386</td>\n","      <td>-0.116900</td>\n","      <td>-0.039909</td>\n","      <td>0.080851</td>\n","      <td>0.214949</td>\n","      <td>-0.190066</td>\n","      <td>0.225420</td>\n","      <td>-0.596441</td>\n","      <td>0.310375</td>\n","      <td>-0.241014</td>\n","      <td>0.458903</td>\n","      <td>-0.351543</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.567978</td>\n","      <td>-2.201131</td>\n","      <td>-0.614922</td>\n","      <td>-0.801349</td>\n","      <td>-2.384910</td>\n","      <td>0.760731</td>\n","      <td>-1.632905</td>\n","      <td>0.302920</td>\n","      <td>0.221511</td>\n","      <td>-0.471476</td>\n","      <td>-0.771419</td>\n","      <td>1.084920</td>\n","      <td>0.631769</td>\n","      <td>-1.710772</td>\n","      <td>1.539223</td>\n","      <td>-0.743246</td>\n","      <td>-0.374833</td>\n","      <td>-1.407724</td>\n","      <td>1.576159</td>\n","      <td>-1.418647</td>\n","      <td>-1.409437</td>\n","      <td>-0.251637</td>\n","      <td>0.275095</td>\n","      <td>2.448019</td>\n","      <td>0.801495</td>\n","      <td>0.588341</td>\n","      <td>-1.772040</td>\n","      <td>1.614959</td>\n","      <td>-1.105157</td>\n","      <td>-0.418067</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.750881</td>\n","      <td>1.498420</td>\n","      <td>-0.005955</td>\n","      <td>-2.605520</td>\n","      <td>0.195694</td>\n","      <td>0.694679</td>\n","      <td>-0.512877</td>\n","      <td>0.191055</td>\n","      <td>-0.278650</td>\n","      <td>0.962479</td>\n","      <td>-0.384375</td>\n","      <td>-1.315325</td>\n","      <td>-0.888294</td>\n","      <td>-1.525370</td>\n","      <td>-0.679713</td>\n","      <td>0.327865</td>\n","      <td>0.539402</td>\n","      <td>0.640949</td>\n","      <td>0.027017</td>\n","      <td>0.139889</td>\n","      <td>-0.573493</td>\n","      <td>-0.873278</td>\n","      <td>-1.793136</td>\n","      <td>1.145714</td>\n","      <td>-0.215344</td>\n","      <td>-1.224568</td>\n","      <td>0.400691</td>\n","      <td>-0.390125</td>\n","      <td>-0.356242</td>\n","      <td>-0.424035</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.004469</td>\n","      <td>-1.149714</td>\n","      <td>0.952337</td>\n","      <td>0.074564</td>\n","      <td>-0.944146</td>\n","      <td>-0.593292</td>\n","      <td>-1.099060</td>\n","      <td>0.108553</td>\n","      <td>0.985424</td>\n","      <td>0.257351</td>\n","      <td>-1.067320</td>\n","      <td>-0.316141</td>\n","      <td>-0.123461</td>\n","      <td>-1.198122</td>\n","      <td>0.821613</td>\n","      <td>0.700608</td>\n","      <td>0.266881</td>\n","      <td>0.316385</td>\n","      <td>-0.966408</td>\n","      <td>-1.514584</td>\n","      <td>-0.373034</td>\n","      <td>-0.167440</td>\n","      <td>-0.390030</td>\n","      <td>0.308194</td>\n","      <td>0.971600</td>\n","      <td>-0.720582</td>\n","      <td>1.720384</td>\n","      <td>0.413393</td>\n","      <td>-0.184562</td>\n","      <td>-0.440304</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.236971</td>\n","      <td>-1.600098</td>\n","      <td>1.377071</td>\n","      <td>-1.841893</td>\n","      <td>-1.524467</td>\n","      <td>2.226337</td>\n","      <td>2.639262</td>\n","      <td>0.511164</td>\n","      <td>1.320116</td>\n","      <td>0.391062</td>\n","      <td>1.137804</td>\n","      <td>-0.309272</td>\n","      <td>0.187227</td>\n","      <td>0.747987</td>\n","      <td>0.098155</td>\n","      <td>1.059413</td>\n","      <td>0.337805</td>\n","      <td>-1.205560</td>\n","      <td>0.366562</td>\n","      <td>0.083761</td>\n","      <td>1.189263</td>\n","      <td>0.286585</td>\n","      <td>0.137673</td>\n","      <td>0.588780</td>\n","      <td>1.756498</td>\n","      <td>0.028408</td>\n","      <td>0.634882</td>\n","      <td>1.645391</td>\n","      <td>3.001880</td>\n","      <td>-0.302106</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>104995</th>\n","      <td>1.242571</td>\n","      <td>-0.724395</td>\n","      <td>1.261111</td>\n","      <td>1.278857</td>\n","      <td>2.322340</td>\n","      <td>-0.490254</td>\n","      <td>0.139270</td>\n","      <td>-0.165526</td>\n","      <td>0.890162</td>\n","      <td>-0.455232</td>\n","      <td>0.242864</td>\n","      <td>-1.276659</td>\n","      <td>0.256462</td>\n","      <td>0.246810</td>\n","      <td>0.267005</td>\n","      <td>0.038862</td>\n","      <td>-0.074259</td>\n","      <td>0.564980</td>\n","      <td>0.279900</td>\n","      <td>1.165863</td>\n","      <td>-0.023549</td>\n","      <td>0.468836</td>\n","      <td>1.054057</td>\n","      <td>0.161023</td>\n","      <td>0.716466</td>\n","      <td>-1.163623</td>\n","      <td>0.552891</td>\n","      <td>0.221145</td>\n","      <td>0.399937</td>\n","      <td>-0.422715</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>104996</th>\n","      <td>0.869350</td>\n","      <td>-0.716189</td>\n","      <td>1.077431</td>\n","      <td>0.442280</td>\n","      <td>-0.086999</td>\n","      <td>-0.390156</td>\n","      <td>-0.553594</td>\n","      <td>0.650130</td>\n","      <td>0.243709</td>\n","      <td>-0.624874</td>\n","      <td>-0.063015</td>\n","      <td>1.126515</td>\n","      <td>1.147006</td>\n","      <td>0.230889</td>\n","      <td>0.644338</td>\n","      <td>-0.341495</td>\n","      <td>-0.137883</td>\n","      <td>-0.068880</td>\n","      <td>0.118798</td>\n","      <td>1.439197</td>\n","      <td>-0.042313</td>\n","      <td>-0.012094</td>\n","      <td>-0.127311</td>\n","      <td>0.416907</td>\n","      <td>0.970739</td>\n","      <td>-2.341691</td>\n","      <td>0.275980</td>\n","      <td>-0.589615</td>\n","      <td>0.577933</td>\n","      <td>-0.132561</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>104997</th>\n","      <td>-1.196623</td>\n","      <td>0.909011</td>\n","      <td>0.293415</td>\n","      <td>-0.898783</td>\n","      <td>0.155704</td>\n","      <td>0.534782</td>\n","      <td>-0.194419</td>\n","      <td>0.301714</td>\n","      <td>-0.282338</td>\n","      <td>-0.109636</td>\n","      <td>-0.122705</td>\n","      <td>0.566040</td>\n","      <td>0.902178</td>\n","      <td>0.880222</td>\n","      <td>-0.423685</td>\n","      <td>0.268529</td>\n","      <td>1.224100</td>\n","      <td>-1.112371</td>\n","      <td>0.713036</td>\n","      <td>0.666920</td>\n","      <td>-0.053854</td>\n","      <td>-0.658566</td>\n","      <td>-1.389845</td>\n","      <td>0.087376</td>\n","      <td>-1.443057</td>\n","      <td>0.512218</td>\n","      <td>0.265093</td>\n","      <td>-0.164843</td>\n","      <td>-0.050306</td>\n","      <td>-0.434148</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>104998</th>\n","      <td>-0.753099</td>\n","      <td>0.682304</td>\n","      <td>-0.056999</td>\n","      <td>-0.018557</td>\n","      <td>0.852925</td>\n","      <td>-0.009247</td>\n","      <td>0.093821</td>\n","      <td>-0.075994</td>\n","      <td>0.042952</td>\n","      <td>0.135231</td>\n","      <td>-0.097980</td>\n","      <td>-0.570460</td>\n","      <td>0.814489</td>\n","      <td>1.386511</td>\n","      <td>-0.146068</td>\n","      <td>1.519815</td>\n","      <td>0.057927</td>\n","      <td>-0.552795</td>\n","      <td>-0.538474</td>\n","      <td>-0.781397</td>\n","      <td>0.143004</td>\n","      <td>0.183935</td>\n","      <td>0.262592</td>\n","      <td>-0.062888</td>\n","      <td>-0.465782</td>\n","      <td>0.484884</td>\n","      <td>-0.983651</td>\n","      <td>0.133288</td>\n","      <td>0.075708</td>\n","      <td>0.099928</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>104999</th>\n","      <td>-1.421452</td>\n","      <td>0.790646</td>\n","      <td>0.195935</td>\n","      <td>0.065631</td>\n","      <td>1.276834</td>\n","      <td>0.000833</td>\n","      <td>-0.132501</td>\n","      <td>-0.062958</td>\n","      <td>-0.234700</td>\n","      <td>1.870792</td>\n","      <td>-0.610759</td>\n","      <td>0.520084</td>\n","      <td>-2.442675</td>\n","      <td>0.872894</td>\n","      <td>1.784634</td>\n","      <td>-0.268339</td>\n","      <td>-0.754464</td>\n","      <td>1.291921</td>\n","      <td>-0.494694</td>\n","      <td>-1.213858</td>\n","      <td>-0.651930</td>\n","      <td>-0.140300</td>\n","      <td>0.447945</td>\n","      <td>-0.079940</td>\n","      <td>0.055602</td>\n","      <td>1.193055</td>\n","      <td>-0.535467</td>\n","      <td>0.029749</td>\n","      <td>-0.053314</td>\n","      <td>-0.383769</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>105000 rows × 31 columns</p>\n","</div>"],"text/plain":["            Time     feat1     feat2     feat3     feat4     feat5     feat6  \\\n","0      -0.605987 -0.579936  1.350908  1.083101  1.991932  0.712025  0.167394   \n","1       0.567978 -2.201131 -0.614922 -0.801349 -2.384910  0.760731 -1.632905   \n","2      -1.750881  1.498420 -0.005955 -2.605520  0.195694  0.694679 -0.512877   \n","3      -0.004469 -1.149714  0.952337  0.074564 -0.944146 -0.593292 -1.099060   \n","4       1.236971 -1.600098  1.377071 -1.841893 -1.524467  2.226337  2.639262   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  1.242571 -0.724395  1.261111  1.278857  2.322340 -0.490254  0.139270   \n","104996  0.869350 -0.716189  1.077431  0.442280 -0.086999 -0.390156 -0.553594   \n","104997 -1.196623  0.909011  0.293415 -0.898783  0.155704  0.534782 -0.194419   \n","104998 -0.753099  0.682304 -0.056999 -0.018557  0.852925 -0.009247  0.093821   \n","104999 -1.421452  0.790646  0.195935  0.065631  1.276834  0.000833 -0.132501   \n","\n","           feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n","0       1.039911 -0.104928 -2.160363  0.921554 -0.427283  0.423795  2.096704   \n","1       0.302920  0.221511 -0.471476 -0.771419  1.084920  0.631769 -1.710772   \n","2       0.191055 -0.278650  0.962479 -0.384375 -1.315325 -0.888294 -1.525370   \n","3       0.108553  0.985424  0.257351 -1.067320 -0.316141 -0.123461 -1.198122   \n","4       0.511164  1.320116  0.391062  1.137804 -0.309272  0.187227  0.747987   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995 -0.165526  0.890162 -0.455232  0.242864 -1.276659  0.256462  0.246810   \n","104996  0.650130  0.243709 -0.624874 -0.063015  1.126515  1.147006  0.230889   \n","104997  0.301714 -0.282338 -0.109636 -0.122705  0.566040  0.902178  0.880222   \n","104998 -0.075994  0.042952  0.135231 -0.097980 -0.570460  0.814489  1.386511   \n","104999 -0.062958 -0.234700  1.870792 -0.610759  0.520084 -2.442675  0.872894   \n","\n","          feat14    feat15    feat16    feat17    feat18    feat19    feat20  \\\n","0       0.003399  1.165734 -0.034925 -0.199062 -0.649386 -0.116900 -0.039909   \n","1       1.539223 -0.743246 -0.374833 -1.407724  1.576159 -1.418647 -1.409437   \n","2      -0.679713  0.327865  0.539402  0.640949  0.027017  0.139889 -0.573493   \n","3       0.821613  0.700608  0.266881  0.316385 -0.966408 -1.514584 -0.373034   \n","4       0.098155  1.059413  0.337805 -1.205560  0.366562  0.083761  1.189263   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  0.267005  0.038862 -0.074259  0.564980  0.279900  1.165863 -0.023549   \n","104996  0.644338 -0.341495 -0.137883 -0.068880  0.118798  1.439197 -0.042313   \n","104997 -0.423685  0.268529  1.224100 -1.112371  0.713036  0.666920 -0.053854   \n","104998 -0.146068  1.519815  0.057927 -0.552795 -0.538474 -0.781397  0.143004   \n","104999  1.784634 -0.268339 -0.754464  1.291921 -0.494694 -1.213858 -0.651930   \n","\n","          feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n","0       0.080851  0.214949 -0.190066  0.225420 -0.596441  0.310375 -0.241014   \n","1      -0.251637  0.275095  2.448019  0.801495  0.588341 -1.772040  1.614959   \n","2      -0.873278 -1.793136  1.145714 -0.215344 -1.224568  0.400691 -0.390125   \n","3      -0.167440 -0.390030  0.308194  0.971600 -0.720582  1.720384  0.413393   \n","4       0.286585  0.137673  0.588780  1.756498  0.028408  0.634882  1.645391   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","104995  0.468836  1.054057  0.161023  0.716466 -1.163623  0.552891  0.221145   \n","104996 -0.012094 -0.127311  0.416907  0.970739 -2.341691  0.275980 -0.589615   \n","104997 -0.658566 -1.389845  0.087376 -1.443057  0.512218  0.265093 -0.164843   \n","104998  0.183935  0.262592 -0.062888 -0.465782  0.484884 -0.983651  0.133288   \n","104999 -0.140300  0.447945 -0.079940  0.055602  1.193055 -0.535467  0.029749   \n","\n","          feat28  Transaction_Amount  IsFraud  \n","0       0.458903           -0.351543        0  \n","1      -1.105157           -0.418067        0  \n","2      -0.356242           -0.424035        0  \n","3      -0.184562           -0.440304        0  \n","4       3.001880           -0.302106        0  \n","...          ...                 ...      ...  \n","104995  0.399937           -0.422715        0  \n","104996  0.577933           -0.132561        0  \n","104997 -0.050306           -0.434148        0  \n","104998  0.075708            0.099928        0  \n","104999 -0.053314           -0.383769        0  \n","\n","[105000 rows x 31 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.914891Z","iopub.status.busy":"2024-03-17T06:46:28.914565Z","iopub.status.idle":"2024-03-17T06:46:28.924094Z","shell.execute_reply":"2024-03-17T06:46:28.923245Z","shell.execute_reply.started":"2024-03-17T06:46:28.914865Z"},"trusted":true},"outputs":[{"data":{"text/plain":["IsFraud\n","0    104794\n","1       206\n","Name: count, dtype: int64"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["train_df.IsFraud.value_counts()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.925689Z","iopub.status.busy":"2024-03-17T06:46:28.925142Z","iopub.status.idle":"2024-03-17T06:46:28.936229Z","shell.execute_reply":"2024-03-17T06:46:28.934913Z","shell.execute_reply.started":"2024-03-17T06:46:28.925660Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of examples to sample:  104588\n"]}],"source":["print(\"Number of examples to sample: \",104794-206)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.939004Z","iopub.status.busy":"2024-03-17T06:46:28.938174Z","iopub.status.idle":"2024-03-17T06:46:28.965167Z","shell.execute_reply":"2024-03-17T06:46:28.964061Z","shell.execute_reply.started":"2024-03-17T06:46:28.938955Z"},"trusted":true},"outputs":[],"source":["oversampled_data = train_df.query(\"IsFraud == 1\").sample(104588, replace=True, random_state = 123)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:28.967382Z","iopub.status.busy":"2024-03-17T06:46:28.966853Z","iopub.status.idle":"2024-03-17T06:46:29.007908Z","shell.execute_reply":"2024-03-17T06:46:29.006769Z","shell.execute_reply.started":"2024-03-17T06:46:28.967344Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>feat1</th>\n","      <th>feat2</th>\n","      <th>feat3</th>\n","      <th>feat4</th>\n","      <th>feat5</th>\n","      <th>feat6</th>\n","      <th>feat7</th>\n","      <th>feat8</th>\n","      <th>feat9</th>\n","      <th>feat10</th>\n","      <th>feat11</th>\n","      <th>feat12</th>\n","      <th>feat13</th>\n","      <th>feat14</th>\n","      <th>feat15</th>\n","      <th>feat16</th>\n","      <th>feat17</th>\n","      <th>feat18</th>\n","      <th>feat19</th>\n","      <th>feat20</th>\n","      <th>feat21</th>\n","      <th>feat22</th>\n","      <th>feat23</th>\n","      <th>feat24</th>\n","      <th>feat25</th>\n","      <th>feat26</th>\n","      <th>feat27</th>\n","      <th>feat28</th>\n","      <th>Transaction_Amount</th>\n","      <th>IsFraud</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>54285</th>\n","      <td>0.058686</td>\n","      <td>0.022721</td>\n","      <td>1.809624</td>\n","      <td>-3.482273</td>\n","      <td>0.947536</td>\n","      <td>1.277439</td>\n","      <td>-1.063422</td>\n","      <td>0.626042</td>\n","      <td>0.436137</td>\n","      <td>-0.318931</td>\n","      <td>-1.728865</td>\n","      <td>1.681081</td>\n","      <td>0.489496</td>\n","      <td>0.731425</td>\n","      <td>-4.902058</td>\n","      <td>0.254112</td>\n","      <td>1.540697</td>\n","      <td>3.843762</td>\n","      <td>2.677371</td>\n","      <td>-0.008916</td>\n","      <td>0.134534</td>\n","      <td>-0.255553</td>\n","      <td>-0.357498</td>\n","      <td>0.295109</td>\n","      <td>-1.769005</td>\n","      <td>-0.957840</td>\n","      <td>-0.834780</td>\n","      <td>0.489430</td>\n","      <td>-0.401790</td>\n","      <td>-0.437478</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>64137</th>\n","      <td>-0.805532</td>\n","      <td>-0.749834</td>\n","      <td>1.422749</td>\n","      <td>0.119348</td>\n","      <td>0.202788</td>\n","      <td>1.072884</td>\n","      <td>-1.414244</td>\n","      <td>1.520531</td>\n","      <td>-0.395838</td>\n","      <td>-0.229562</td>\n","      <td>-1.570938</td>\n","      <td>-0.095053</td>\n","      <td>-0.818974</td>\n","      <td>-0.594457</td>\n","      <td>-3.496443</td>\n","      <td>0.541810</td>\n","      <td>1.439622</td>\n","      <td>1.052520</td>\n","      <td>1.450854</td>\n","      <td>-1.890807</td>\n","      <td>-0.447021</td>\n","      <td>-0.593762</td>\n","      <td>-1.512232</td>\n","      <td>-0.555748</td>\n","      <td>0.206981</td>\n","      <td>0.577676</td>\n","      <td>-1.711922</td>\n","      <td>0.433045</td>\n","      <td>0.722018</td>\n","      <td>-0.440304</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35236</th>\n","      <td>1.142612</td>\n","      <td>0.916153</td>\n","      <td>0.120656</td>\n","      <td>-0.908037</td>\n","      <td>0.276177</td>\n","      <td>0.305358</td>\n","      <td>-0.894738</td>\n","      <td>0.699796</td>\n","      <td>-0.466159</td>\n","      <td>0.107524</td>\n","      <td>-0.028267</td>\n","      <td>-0.842089</td>\n","      <td>-0.859641</td>\n","      <td>-1.775759</td>\n","      <td>1.218571</td>\n","      <td>1.138755</td>\n","      <td>-0.063811</td>\n","      <td>-0.150212</td>\n","      <td>-1.087867</td>\n","      <td>0.087998</td>\n","      <td>-0.312374</td>\n","      <td>-1.148318</td>\n","      <td>-2.884532</td>\n","      <td>0.646479</td>\n","      <td>-0.301311</td>\n","      <td>0.016395</td>\n","      <td>-0.004893</td>\n","      <td>-0.374500</td>\n","      <td>-0.150020</td>\n","      <td>-0.208570</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>50338</th>\n","      <td>-1.408117</td>\n","      <td>-3.688421</td>\n","      <td>3.046550</td>\n","      <td>-6.886956</td>\n","      <td>1.952436</td>\n","      <td>-1.420187</td>\n","      <td>-2.854983</td>\n","      <td>-0.806345</td>\n","      <td>3.075779</td>\n","      <td>1.268887</td>\n","      <td>-1.702097</td>\n","      <td>1.209031</td>\n","      <td>-3.246641</td>\n","      <td>0.070676</td>\n","      <td>-4.251975</td>\n","      <td>0.145501</td>\n","      <td>2.455250</td>\n","      <td>7.029105</td>\n","      <td>2.448707</td>\n","      <td>-1.233095</td>\n","      <td>0.009357</td>\n","      <td>-0.514813</td>\n","      <td>-2.454005</td>\n","      <td>1.201763</td>\n","      <td>1.169675</td>\n","      <td>-0.413895</td>\n","      <td>-1.118037</td>\n","      <td>-0.254205</td>\n","      <td>-2.369955</td>\n","      <td>0.118710</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>-2.452625</td>\n","      <td>-0.586710</td>\n","      <td>0.915419</td>\n","      <td>0.883507</td>\n","      <td>-0.326297</td>\n","      <td>1.108902</td>\n","      <td>-0.609927</td>\n","      <td>1.684005</td>\n","      <td>-0.665415</td>\n","      <td>0.668965</td>\n","      <td>-0.927247</td>\n","      <td>1.566142</td>\n","      <td>-2.756475</td>\n","      <td>0.518203</td>\n","      <td>2.415373</td>\n","      <td>-1.569250</td>\n","      <td>0.594214</td>\n","      <td>-0.492240</td>\n","      <td>0.933604</td>\n","      <td>-1.145739</td>\n","      <td>-0.252305</td>\n","      <td>-0.292334</td>\n","      <td>-0.058235</td>\n","      <td>-0.615601</td>\n","      <td>-0.110324</td>\n","      <td>0.568066</td>\n","      <td>-1.395559</td>\n","      <td>-0.829967</td>\n","      <td>-0.384170</td>\n","      <td>-0.331002</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>88078</th>\n","      <td>0.556030</td>\n","      <td>0.874831</td>\n","      <td>-0.142221</td>\n","      <td>-0.548469</td>\n","      <td>-0.313002</td>\n","      <td>0.270156</td>\n","      <td>0.014830</td>\n","      <td>-0.122761</td>\n","      <td>-0.163679</td>\n","      <td>0.172825</td>\n","      <td>-0.020580</td>\n","      <td>0.155890</td>\n","      <td>0.967554</td>\n","      <td>1.221262</td>\n","      <td>0.089738</td>\n","      <td>0.597747</td>\n","      <td>1.135555</td>\n","      <td>-1.551537</td>\n","      <td>0.772993</td>\n","      <td>0.824774</td>\n","      <td>0.235249</td>\n","      <td>0.019788</td>\n","      <td>-0.001267</td>\n","      <td>-0.408322</td>\n","      <td>-1.614731</td>\n","      <td>0.453061</td>\n","      <td>2.391637</td>\n","      <td>-0.398864</td>\n","      <td>-0.098181</td>\n","      <td>-0.159070</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43155</th>\n","      <td>1.196059</td>\n","      <td>0.876567</td>\n","      <td>-0.381948</td>\n","      <td>-0.741361</td>\n","      <td>0.126699</td>\n","      <td>-0.317736</td>\n","      <td>-0.689131</td>\n","      <td>0.037364</td>\n","      <td>-0.640880</td>\n","      <td>-0.818101</td>\n","      <td>0.934737</td>\n","      <td>-1.255735</td>\n","      <td>0.310235</td>\n","      <td>1.399481</td>\n","      <td>-0.156203</td>\n","      <td>0.676301</td>\n","      <td>-1.790447</td>\n","      <td>-0.731307</td>\n","      <td>1.759721</td>\n","      <td>-1.102885</td>\n","      <td>-1.099972</td>\n","      <td>-0.750069</td>\n","      <td>-0.846753</td>\n","      <td>0.217362</td>\n","      <td>-0.301145</td>\n","      <td>0.656759</td>\n","      <td>-1.644602</td>\n","      <td>0.181449</td>\n","      <td>0.086716</td>\n","      <td>-0.069994</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10748</th>\n","      <td>0.471219</td>\n","      <td>-0.606132</td>\n","      <td>0.709006</td>\n","      <td>0.105073</td>\n","      <td>-0.314187</td>\n","      <td>1.246549</td>\n","      <td>-0.763648</td>\n","      <td>-4.000269</td>\n","      <td>-4.102510</td>\n","      <td>0.493284</td>\n","      <td>-0.436561</td>\n","      <td>0.694947</td>\n","      <td>1.999372</td>\n","      <td>0.964981</td>\n","      <td>-0.171645</td>\n","      <td>-1.005450</td>\n","      <td>-0.774678</td>\n","      <td>0.014954</td>\n","      <td>-0.979067</td>\n","      <td>1.812123</td>\n","      <td>0.716517</td>\n","      <td>-1.906629</td>\n","      <td>0.408747</td>\n","      <td>-7.110856</td>\n","      <td>0.316684</td>\n","      <td>-3.256729</td>\n","      <td>-0.229329</td>\n","      <td>1.637716</td>\n","      <td>1.499327</td>\n","      <td>-0.434023</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>96987</th>\n","      <td>-2.230889</td>\n","      <td>-0.655774</td>\n","      <td>0.490499</td>\n","      <td>0.192601</td>\n","      <td>0.637990</td>\n","      <td>0.675041</td>\n","      <td>0.067982</td>\n","      <td>0.694146</td>\n","      <td>-0.165385</td>\n","      <td>0.839060</td>\n","      <td>-0.099414</td>\n","      <td>1.941955</td>\n","      <td>-1.918601</td>\n","      <td>1.996333</td>\n","      <td>2.066786</td>\n","      <td>0.036722</td>\n","      <td>-0.512646</td>\n","      <td>0.660499</td>\n","      <td>0.891842</td>\n","      <td>0.385374</td>\n","      <td>-0.309619</td>\n","      <td>0.150497</td>\n","      <td>1.235953</td>\n","      <td>0.882567</td>\n","      <td>-0.634195</td>\n","      <td>-0.832925</td>\n","      <td>-0.515638</td>\n","      <td>0.394574</td>\n","      <td>0.611249</td>\n","      <td>-0.111140</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>90586</th>\n","      <td>-2.158133</td>\n","      <td>0.783789</td>\n","      <td>0.218094</td>\n","      <td>-0.377616</td>\n","      <td>0.853499</td>\n","      <td>0.303612</td>\n","      <td>0.104422</td>\n","      <td>-0.060898</td>\n","      <td>-0.028952</td>\n","      <td>1.263943</td>\n","      <td>-0.105412</td>\n","      <td>1.815936</td>\n","      <td>-2.414391</td>\n","      <td>0.431312</td>\n","      <td>2.719791</td>\n","      <td>-0.177174</td>\n","      <td>-0.027575</td>\n","      <td>0.511491</td>\n","      <td>0.247069</td>\n","      <td>-1.112548</td>\n","      <td>-0.656005</td>\n","      <td>-0.044774</td>\n","      <td>0.284770</td>\n","      <td>-0.085323</td>\n","      <td>-0.665805</td>\n","      <td>1.004732</td>\n","      <td>-0.738332</td>\n","      <td>-0.032649</td>\n","      <td>-0.128385</td>\n","      <td>-0.383769</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>104588 rows × 31 columns</p>\n","</div>"],"text/plain":["           Time     feat1     feat2     feat3     feat4     feat5     feat6  \\\n","54285  0.058686  0.022721  1.809624 -3.482273  0.947536  1.277439 -1.063422   \n","64137 -0.805532 -0.749834  1.422749  0.119348  0.202788  1.072884 -1.414244   \n","35236  1.142612  0.916153  0.120656 -0.908037  0.276177  0.305358 -0.894738   \n","50338 -1.408117 -3.688421  3.046550 -6.886956  1.952436 -1.420187 -2.854983   \n","7040  -2.452625 -0.586710  0.915419  0.883507 -0.326297  1.108902 -0.609927   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","88078  0.556030  0.874831 -0.142221 -0.548469 -0.313002  0.270156  0.014830   \n","43155  1.196059  0.876567 -0.381948 -0.741361  0.126699 -0.317736 -0.689131   \n","10748  0.471219 -0.606132  0.709006  0.105073 -0.314187  1.246549 -0.763648   \n","96987 -2.230889 -0.655774  0.490499  0.192601  0.637990  0.675041  0.067982   \n","90586 -2.158133  0.783789  0.218094 -0.377616  0.853499  0.303612  0.104422   \n","\n","          feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n","54285  0.626042  0.436137 -0.318931 -1.728865  1.681081  0.489496  0.731425   \n","64137  1.520531 -0.395838 -0.229562 -1.570938 -0.095053 -0.818974 -0.594457   \n","35236  0.699796 -0.466159  0.107524 -0.028267 -0.842089 -0.859641 -1.775759   \n","50338 -0.806345  3.075779  1.268887 -1.702097  1.209031 -3.246641  0.070676   \n","7040   1.684005 -0.665415  0.668965 -0.927247  1.566142 -2.756475  0.518203   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","88078 -0.122761 -0.163679  0.172825 -0.020580  0.155890  0.967554  1.221262   \n","43155  0.037364 -0.640880 -0.818101  0.934737 -1.255735  0.310235  1.399481   \n","10748 -4.000269 -4.102510  0.493284 -0.436561  0.694947  1.999372  0.964981   \n","96987  0.694146 -0.165385  0.839060 -0.099414  1.941955 -1.918601  1.996333   \n","90586 -0.060898 -0.028952  1.263943 -0.105412  1.815936 -2.414391  0.431312   \n","\n","         feat14    feat15    feat16    feat17    feat18    feat19    feat20  \\\n","54285 -4.902058  0.254112  1.540697  3.843762  2.677371 -0.008916  0.134534   \n","64137 -3.496443  0.541810  1.439622  1.052520  1.450854 -1.890807 -0.447021   \n","35236  1.218571  1.138755 -0.063811 -0.150212 -1.087867  0.087998 -0.312374   \n","50338 -4.251975  0.145501  2.455250  7.029105  2.448707 -1.233095  0.009357   \n","7040   2.415373 -1.569250  0.594214 -0.492240  0.933604 -1.145739 -0.252305   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","88078  0.089738  0.597747  1.135555 -1.551537  0.772993  0.824774  0.235249   \n","43155 -0.156203  0.676301 -1.790447 -0.731307  1.759721 -1.102885 -1.099972   \n","10748 -0.171645 -1.005450 -0.774678  0.014954 -0.979067  1.812123  0.716517   \n","96987  2.066786  0.036722 -0.512646  0.660499  0.891842  0.385374 -0.309619   \n","90586  2.719791 -0.177174 -0.027575  0.511491  0.247069 -1.112548 -0.656005   \n","\n","         feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n","54285 -0.255553 -0.357498  0.295109 -1.769005 -0.957840 -0.834780  0.489430   \n","64137 -0.593762 -1.512232 -0.555748  0.206981  0.577676 -1.711922  0.433045   \n","35236 -1.148318 -2.884532  0.646479 -0.301311  0.016395 -0.004893 -0.374500   \n","50338 -0.514813 -2.454005  1.201763  1.169675 -0.413895 -1.118037 -0.254205   \n","7040  -0.292334 -0.058235 -0.615601 -0.110324  0.568066 -1.395559 -0.829967   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","88078  0.019788 -0.001267 -0.408322 -1.614731  0.453061  2.391637 -0.398864   \n","43155 -0.750069 -0.846753  0.217362 -0.301145  0.656759 -1.644602  0.181449   \n","10748 -1.906629  0.408747 -7.110856  0.316684 -3.256729 -0.229329  1.637716   \n","96987  0.150497  1.235953  0.882567 -0.634195 -0.832925 -0.515638  0.394574   \n","90586 -0.044774  0.284770 -0.085323 -0.665805  1.004732 -0.738332 -0.032649   \n","\n","         feat28  Transaction_Amount  IsFraud  \n","54285 -0.401790           -0.437478        1  \n","64137  0.722018           -0.440304        1  \n","35236 -0.150020           -0.208570        1  \n","50338 -2.369955            0.118710        1  \n","7040  -0.384170           -0.331002        1  \n","...         ...                 ...      ...  \n","88078 -0.098181           -0.159070        1  \n","43155  0.086716           -0.069994        1  \n","10748  1.499327           -0.434023        1  \n","96987  0.611249           -0.111140        1  \n","90586 -0.128385           -0.383769        1  \n","\n","[104588 rows x 31 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["oversampled_data"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.009540Z","iopub.status.busy":"2024-03-17T06:46:29.009191Z","iopub.status.idle":"2024-03-17T06:46:29.120303Z","shell.execute_reply":"2024-03-17T06:46:29.119255Z","shell.execute_reply.started":"2024-03-17T06:46:29.009511Z"},"trusted":true},"outputs":[],"source":["train_df = pd.concat([train_df, oversampled_data], axis = 0).sample(frac=1.0, random_state = 123).reset_index(drop=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.122070Z","iopub.status.busy":"2024-03-17T06:46:29.121591Z","iopub.status.idle":"2024-03-17T06:46:29.162046Z","shell.execute_reply":"2024-03-17T06:46:29.161264Z","shell.execute_reply.started":"2024-03-17T06:46:29.122029Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>feat1</th>\n","      <th>feat2</th>\n","      <th>feat3</th>\n","      <th>feat4</th>\n","      <th>feat5</th>\n","      <th>feat6</th>\n","      <th>feat7</th>\n","      <th>feat8</th>\n","      <th>feat9</th>\n","      <th>feat10</th>\n","      <th>feat11</th>\n","      <th>feat12</th>\n","      <th>feat13</th>\n","      <th>feat14</th>\n","      <th>feat15</th>\n","      <th>feat16</th>\n","      <th>feat17</th>\n","      <th>feat18</th>\n","      <th>feat19</th>\n","      <th>feat20</th>\n","      <th>feat21</th>\n","      <th>feat22</th>\n","      <th>feat23</th>\n","      <th>feat24</th>\n","      <th>feat25</th>\n","      <th>feat26</th>\n","      <th>feat27</th>\n","      <th>feat28</th>\n","      <th>Transaction_Amount</th>\n","      <th>IsFraud</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.768110</td>\n","      <td>0.939769</td>\n","      <td>1.223700</td>\n","      <td>-2.991152</td>\n","      <td>1.123012</td>\n","      <td>1.704541</td>\n","      <td>-1.328989</td>\n","      <td>1.371673</td>\n","      <td>-0.784621</td>\n","      <td>-0.828324</td>\n","      <td>-1.259889</td>\n","      <td>0.001567</td>\n","      <td>-0.495481</td>\n","      <td>0.693734</td>\n","      <td>-4.271886</td>\n","      <td>1.054237</td>\n","      <td>1.350543</td>\n","      <td>3.756158</td>\n","      <td>1.696644</td>\n","      <td>-0.650645</td>\n","      <td>-0.051826</td>\n","      <td>-0.795753</td>\n","      <td>-1.470152</td>\n","      <td>-0.863940</td>\n","      <td>-1.241771</td>\n","      <td>1.760455</td>\n","      <td>-0.752460</td>\n","      <td>0.100848</td>\n","      <td>0.384444</td>\n","      <td>-0.446586</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.349230</td>\n","      <td>-0.568117</td>\n","      <td>0.140197</td>\n","      <td>0.609375</td>\n","      <td>0.278536</td>\n","      <td>1.189459</td>\n","      <td>-1.004200</td>\n","      <td>0.566494</td>\n","      <td>-0.553873</td>\n","      <td>-0.163866</td>\n","      <td>-0.073509</td>\n","      <td>-1.107040</td>\n","      <td>-0.036328</td>\n","      <td>0.542504</td>\n","      <td>0.019257</td>\n","      <td>0.977296</td>\n","      <td>0.067737</td>\n","      <td>-0.906713</td>\n","      <td>0.122578</td>\n","      <td>0.722841</td>\n","      <td>0.691264</td>\n","      <td>0.097782</td>\n","      <td>-0.071813</td>\n","      <td>0.384666</td>\n","      <td>-0.166691</td>\n","      <td>-1.327179</td>\n","      <td>0.347896</td>\n","      <td>-0.182021</td>\n","      <td>0.272659</td>\n","      <td>-0.383957</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.466685</td>\n","      <td>-7.194154</td>\n","      <td>-9.485379</td>\n","      <td>-0.845367</td>\n","      <td>2.836798</td>\n","      <td>8.686781</td>\n","      <td>-5.019205</td>\n","      <td>-5.560278</td>\n","      <td>2.203462</td>\n","      <td>0.281144</td>\n","      <td>-0.428504</td>\n","      <td>-0.978458</td>\n","      <td>1.272216</td>\n","      <td>0.236113</td>\n","      <td>1.616980</td>\n","      <td>0.463410</td>\n","      <td>0.901007</td>\n","      <td>-0.477715</td>\n","      <td>-0.236584</td>\n","      <td>-1.450915</td>\n","      <td>6.677298</td>\n","      <td>0.419059</td>\n","      <td>-3.470780</td>\n","      <td>0.721102</td>\n","      <td>-0.078594</td>\n","      <td>1.228407</td>\n","      <td>-1.564535</td>\n","      <td>0.227325</td>\n","      <td>-13.403290</td>\n","      <td>-0.163907</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.058814</td>\n","      <td>0.113543</td>\n","      <td>1.401816</td>\n","      <td>-1.398733</td>\n","      <td>0.670482</td>\n","      <td>1.450409</td>\n","      <td>-0.807429</td>\n","      <td>1.300713</td>\n","      <td>-0.200072</td>\n","      <td>-0.436254</td>\n","      <td>-1.670265</td>\n","      <td>2.306033</td>\n","      <td>-0.751959</td>\n","      <td>-0.862359</td>\n","      <td>-4.346613</td>\n","      <td>0.600386</td>\n","      <td>1.228614</td>\n","      <td>3.521505</td>\n","      <td>1.797972</td>\n","      <td>-0.989358</td>\n","      <td>-0.177573</td>\n","      <td>-0.069301</td>\n","      <td>0.103434</td>\n","      <td>-0.121437</td>\n","      <td>-0.862819</td>\n","      <td>-1.602747</td>\n","      <td>-0.957861</td>\n","      <td>0.406564</td>\n","      <td>0.268752</td>\n","      <td>-0.422590</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.373926</td>\n","      <td>0.885309</td>\n","      <td>1.089171</td>\n","      <td>-2.614838</td>\n","      <td>1.007637</td>\n","      <td>1.432354</td>\n","      <td>-1.012440</td>\n","      <td>0.971439</td>\n","      <td>-0.452535</td>\n","      <td>-0.719845</td>\n","      <td>-1.704646</td>\n","      <td>1.236020</td>\n","      <td>-0.395567</td>\n","      <td>-0.589565</td>\n","      <td>-4.199498</td>\n","      <td>0.416009</td>\n","      <td>1.864389</td>\n","      <td>3.859718</td>\n","      <td>2.849877</td>\n","      <td>-0.305425</td>\n","      <td>-0.042521</td>\n","      <td>-0.451485</td>\n","      <td>-0.908769</td>\n","      <td>-0.782394</td>\n","      <td>-1.431546</td>\n","      <td>1.967089</td>\n","      <td>-0.591978</td>\n","      <td>0.197175</td>\n","      <td>0.411561</td>\n","      <td>-0.440995</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>209583</th>\n","      <td>-0.338432</td>\n","      <td>-0.316089</td>\n","      <td>-0.118815</td>\n","      <td>1.667461</td>\n","      <td>-1.107480</td>\n","      <td>-0.674397</td>\n","      <td>-0.370962</td>\n","      <td>-0.221683</td>\n","      <td>-0.618173</td>\n","      <td>-0.538494</td>\n","      <td>0.004381</td>\n","      <td>-1.270999</td>\n","      <td>0.015001</td>\n","      <td>1.139152</td>\n","      <td>-1.135742</td>\n","      <td>-0.056254</td>\n","      <td>-1.117685</td>\n","      <td>-1.258368</td>\n","      <td>1.967951</td>\n","      <td>-2.666053</td>\n","      <td>-1.115533</td>\n","      <td>0.191648</td>\n","      <td>1.541545</td>\n","      <td>-0.094538</td>\n","      <td>0.641792</td>\n","      <td>-1.360048</td>\n","      <td>-1.174614</td>\n","      <td>0.674398</td>\n","      <td>-0.177117</td>\n","      <td>-0.207879</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>209584</th>\n","      <td>-0.324724</td>\n","      <td>-3.814750</td>\n","      <td>-9.350769</td>\n","      <td>-2.891158</td>\n","      <td>1.745811</td>\n","      <td>-3.629168</td>\n","      <td>1.558720</td>\n","      <td>4.946696</td>\n","      <td>-0.669447</td>\n","      <td>1.083149</td>\n","      <td>-2.066226</td>\n","      <td>1.348931</td>\n","      <td>1.967647</td>\n","      <td>1.356565</td>\n","      <td>-0.207547</td>\n","      <td>1.602725</td>\n","      <td>-0.977176</td>\n","      <td>0.718911</td>\n","      <td>0.187381</td>\n","      <td>-1.601869</td>\n","      <td>15.592342</td>\n","      <td>4.937905</td>\n","      <td>-1.256744</td>\n","      <td>-8.412635</td>\n","      <td>-0.072119</td>\n","      <td>-2.147468</td>\n","      <td>-0.870859</td>\n","      <td>-2.330294</td>\n","      <td>3.260340</td>\n","      <td>15.257833</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>209585</th>\n","      <td>1.220809</td>\n","      <td>0.796147</td>\n","      <td>-0.099002</td>\n","      <td>0.547422</td>\n","      <td>0.914011</td>\n","      <td>-0.636169</td>\n","      <td>0.185766</td>\n","      <td>-0.720636</td>\n","      <td>0.198856</td>\n","      <td>0.878753</td>\n","      <td>-0.028638</td>\n","      <td>0.494145</td>\n","      <td>1.081122</td>\n","      <td>-0.323662</td>\n","      <td>-0.247840</td>\n","      <td>-0.811559</td>\n","      <td>0.181287</td>\n","      <td>-0.538812</td>\n","      <td>0.682941</td>\n","      <td>0.006122</td>\n","      <td>-0.422537</td>\n","      <td>0.066522</td>\n","      <td>0.429318</td>\n","      <td>0.095149</td>\n","      <td>0.003658</td>\n","      <td>0.577299</td>\n","      <td>-0.911813</td>\n","      <td>0.284526</td>\n","      <td>0.045605</td>\n","      <td>-0.446523</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>209586</th>\n","      <td>0.947333</td>\n","      <td>0.563703</td>\n","      <td>-1.643420</td>\n","      <td>0.487272</td>\n","      <td>-0.217162</td>\n","      <td>-1.798818</td>\n","      <td>0.441024</td>\n","      <td>-1.568757</td>\n","      <td>0.245274</td>\n","      <td>0.452362</td>\n","      <td>0.413039</td>\n","      <td>-0.415162</td>\n","      <td>-0.165083</td>\n","      <td>0.117754</td>\n","      <td>-1.520898</td>\n","      <td>-2.388139</td>\n","      <td>1.153945</td>\n","      <td>0.682477</td>\n","      <td>-0.740809</td>\n","      <td>1.854566</td>\n","      <td>1.057514</td>\n","      <td>0.865677</td>\n","      <td>1.236543</td>\n","      <td>-0.609190</td>\n","      <td>0.470295</td>\n","      <td>-0.208343</td>\n","      <td>-0.181393</td>\n","      <td>0.177186</td>\n","      <td>0.244067</td>\n","      <td>1.083024</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>209587</th>\n","      <td>0.628999</td>\n","      <td>-2.400688</td>\n","      <td>-1.753100</td>\n","      <td>1.234112</td>\n","      <td>-0.183952</td>\n","      <td>1.665320</td>\n","      <td>-1.105824</td>\n","      <td>-1.127421</td>\n","      <td>0.655223</td>\n","      <td>-0.121367</td>\n","      <td>-1.158757</td>\n","      <td>0.604150</td>\n","      <td>0.660708</td>\n","      <td>-0.261136</td>\n","      <td>0.470127</td>\n","      <td>0.130604</td>\n","      <td>1.019014</td>\n","      <td>-0.929934</td>\n","      <td>0.956508</td>\n","      <td>0.334918</td>\n","      <td>1.300697</td>\n","      <td>0.783072</td>\n","      <td>-0.177865</td>\n","      <td>-0.860974</td>\n","      <td>0.066864</td>\n","      <td>0.328294</td>\n","      <td>1.924296</td>\n","      <td>-0.812329</td>\n","      <td>-3.442554</td>\n","      <td>-0.232064</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>209588 rows × 31 columns</p>\n","</div>"],"text/plain":["            Time     feat1     feat2     feat3     feat4     feat5     feat6  \\\n","0       0.768110  0.939769  1.223700 -2.991152  1.123012  1.704541 -1.328989   \n","1       0.349230 -0.568117  0.140197  0.609375  0.278536  1.189459 -1.004200   \n","2       0.466685 -7.194154 -9.485379 -0.845367  2.836798  8.686781 -5.019205   \n","3       1.058814  0.113543  1.401816 -1.398733  0.670482  1.450409 -0.807429   \n","4       0.373926  0.885309  1.089171 -2.614838  1.007637  1.432354 -1.012440   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","209583 -0.338432 -0.316089 -0.118815  1.667461 -1.107480 -0.674397 -0.370962   \n","209584 -0.324724 -3.814750 -9.350769 -2.891158  1.745811 -3.629168  1.558720   \n","209585  1.220809  0.796147 -0.099002  0.547422  0.914011 -0.636169  0.185766   \n","209586  0.947333  0.563703 -1.643420  0.487272 -0.217162 -1.798818  0.441024   \n","209587  0.628999 -2.400688 -1.753100  1.234112 -0.183952  1.665320 -1.105824   \n","\n","           feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n","0       1.371673 -0.784621 -0.828324 -1.259889  0.001567 -0.495481  0.693734   \n","1       0.566494 -0.553873 -0.163866 -0.073509 -1.107040 -0.036328  0.542504   \n","2      -5.560278  2.203462  0.281144 -0.428504 -0.978458  1.272216  0.236113   \n","3       1.300713 -0.200072 -0.436254 -1.670265  2.306033 -0.751959 -0.862359   \n","4       0.971439 -0.452535 -0.719845 -1.704646  1.236020 -0.395567 -0.589565   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","209583 -0.221683 -0.618173 -0.538494  0.004381 -1.270999  0.015001  1.139152   \n","209584  4.946696 -0.669447  1.083149 -2.066226  1.348931  1.967647  1.356565   \n","209585 -0.720636  0.198856  0.878753 -0.028638  0.494145  1.081122 -0.323662   \n","209586 -1.568757  0.245274  0.452362  0.413039 -0.415162 -0.165083  0.117754   \n","209587 -1.127421  0.655223 -0.121367 -1.158757  0.604150  0.660708 -0.261136   \n","\n","          feat14    feat15    feat16    feat17    feat18    feat19     feat20  \\\n","0      -4.271886  1.054237  1.350543  3.756158  1.696644 -0.650645  -0.051826   \n","1       0.019257  0.977296  0.067737 -0.906713  0.122578  0.722841   0.691264   \n","2       1.616980  0.463410  0.901007 -0.477715 -0.236584 -1.450915   6.677298   \n","3      -4.346613  0.600386  1.228614  3.521505  1.797972 -0.989358  -0.177573   \n","4      -4.199498  0.416009  1.864389  3.859718  2.849877 -0.305425  -0.042521   \n","...          ...       ...       ...       ...       ...       ...        ...   \n","209583 -1.135742 -0.056254 -1.117685 -1.258368  1.967951 -2.666053  -1.115533   \n","209584 -0.207547  1.602725 -0.977176  0.718911  0.187381 -1.601869  15.592342   \n","209585 -0.247840 -0.811559  0.181287 -0.538812  0.682941  0.006122  -0.422537   \n","209586 -1.520898 -2.388139  1.153945  0.682477 -0.740809  1.854566   1.057514   \n","209587  0.470127  0.130604  1.019014 -0.929934  0.956508  0.334918   1.300697   \n","\n","          feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n","0      -0.795753 -1.470152 -0.863940 -1.241771  1.760455 -0.752460  0.100848   \n","1       0.097782 -0.071813  0.384666 -0.166691 -1.327179  0.347896 -0.182021   \n","2       0.419059 -3.470780  0.721102 -0.078594  1.228407 -1.564535  0.227325   \n","3      -0.069301  0.103434 -0.121437 -0.862819 -1.602747 -0.957861  0.406564   \n","4      -0.451485 -0.908769 -0.782394 -1.431546  1.967089 -0.591978  0.197175   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","209583  0.191648  1.541545 -0.094538  0.641792 -1.360048 -1.174614  0.674398   \n","209584  4.937905 -1.256744 -8.412635 -0.072119 -2.147468 -0.870859 -2.330294   \n","209585  0.066522  0.429318  0.095149  0.003658  0.577299 -0.911813  0.284526   \n","209586  0.865677  1.236543 -0.609190  0.470295 -0.208343 -0.181393  0.177186   \n","209587  0.783072 -0.177865 -0.860974  0.066864  0.328294  1.924296 -0.812329   \n","\n","           feat28  Transaction_Amount  IsFraud  \n","0        0.384444           -0.446586        1  \n","1        0.272659           -0.383957        1  \n","2      -13.403290           -0.163907        1  \n","3        0.268752           -0.422590        1  \n","4        0.411561           -0.440995        1  \n","...           ...                 ...      ...  \n","209583  -0.177117           -0.207879        1  \n","209584   3.260340           15.257833        1  \n","209585   0.045605           -0.446523        0  \n","209586   0.244067            1.083024        0  \n","209587  -3.442554           -0.232064        0  \n","\n","[209588 rows x 31 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.163735Z","iopub.status.busy":"2024-03-17T06:46:29.163234Z","iopub.status.idle":"2024-03-17T06:46:29.173428Z","shell.execute_reply":"2024-03-17T06:46:29.172168Z","shell.execute_reply.started":"2024-03-17T06:46:29.163706Z"},"trusted":true},"outputs":[{"data":{"text/plain":["IsFraud\n","1    104794\n","0    104794\n","Name: count, dtype: int64"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["train_df.IsFraud.value_counts()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.175881Z","iopub.status.busy":"2024-03-17T06:46:29.174693Z","iopub.status.idle":"2024-03-17T06:46:29.230999Z","shell.execute_reply":"2024-03-17T06:46:29.229726Z","shell.execute_reply.started":"2024-03-17T06:46:29.175841Z"},"trusted":true},"outputs":[],"source":["y_train = train_df['IsFraud'].copy()\n","X_train = train_df.drop(['IsFraud'], axis=1).copy()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.233177Z","iopub.status.busy":"2024-03-17T06:46:29.232615Z","iopub.status.idle":"2024-03-17T06:46:29.240450Z","shell.execute_reply":"2024-03-17T06:46:29.239195Z","shell.execute_reply.started":"2024-03-17T06:46:29.233137Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(209588, 30)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["### PyCarret"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from pycaret.classification import *"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_ef875_row8_col1 {\n","  background-color: lightgreen;\n","}\n","</style>\n","<table id=\"T_ef875\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_ef875_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n","      <th id=\"T_ef875_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_ef875_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_ef875_row0_col0\" class=\"data row0 col0\" >Session id</td>\n","      <td id=\"T_ef875_row0_col1\" class=\"data row0 col1\" >1311</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_ef875_row1_col0\" class=\"data row1 col0\" >Target</td>\n","      <td id=\"T_ef875_row1_col1\" class=\"data row1 col1\" >IsFraud</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_ef875_row2_col0\" class=\"data row2 col0\" >Target type</td>\n","      <td id=\"T_ef875_row2_col1\" class=\"data row2 col1\" >Binary</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_ef875_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n","      <td id=\"T_ef875_row3_col1\" class=\"data row3 col1\" >(209588, 31)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_ef875_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n","      <td id=\"T_ef875_row4_col1\" class=\"data row4 col1\" >(209588, 31)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","      <td id=\"T_ef875_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n","      <td id=\"T_ef875_row5_col1\" class=\"data row5 col1\" >(146711, 31)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","      <td id=\"T_ef875_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n","      <td id=\"T_ef875_row6_col1\" class=\"data row6 col1\" >(62877, 31)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","      <td id=\"T_ef875_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n","      <td id=\"T_ef875_row7_col1\" class=\"data row7 col1\" >30</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","      <td id=\"T_ef875_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n","      <td id=\"T_ef875_row8_col1\" class=\"data row8 col1\" >True</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","      <td id=\"T_ef875_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n","      <td id=\"T_ef875_row9_col1\" class=\"data row9 col1\" >simple</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n","      <td id=\"T_ef875_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n","      <td id=\"T_ef875_row10_col1\" class=\"data row10 col1\" >mean</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n","      <td id=\"T_ef875_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n","      <td id=\"T_ef875_row11_col1\" class=\"data row11 col1\" >mode</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n","      <td id=\"T_ef875_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n","      <td id=\"T_ef875_row12_col1\" class=\"data row12 col1\" >StratifiedKFold</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n","      <td id=\"T_ef875_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n","      <td id=\"T_ef875_row13_col1\" class=\"data row13 col1\" >10</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n","      <td id=\"T_ef875_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n","      <td id=\"T_ef875_row14_col1\" class=\"data row14 col1\" >-1</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n","      <td id=\"T_ef875_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n","      <td id=\"T_ef875_row15_col1\" class=\"data row15 col1\" >False</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n","      <td id=\"T_ef875_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n","      <td id=\"T_ef875_row16_col1\" class=\"data row16 col1\" >False</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n","      <td id=\"T_ef875_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n","      <td id=\"T_ef875_row17_col1\" class=\"data row17 col1\" >clf-default-name</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ef875_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n","      <td id=\"T_ef875_row18_col0\" class=\"data row18 col0\" >USI</td>\n","      <td id=\"T_ef875_row18_col1\" class=\"data row18 col1\" >f50b</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x1ebea18e710>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\internal\\metrics.py:51: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n","  warnings.warn(\n"]}],"source":["_ = setup(data=pd.concat([X_train, y_train], axis=1), target='IsFraud')"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style type=\"text/css\">\n","#T_1b05f th {\n","  text-align: left;\n","}\n","#T_1b05f_row0_col0, #T_1b05f_row1_col0, #T_1b05f_row2_col0, #T_1b05f_row2_col1, #T_1b05f_row2_col2, #T_1b05f_row2_col4, #T_1b05f_row2_col5, #T_1b05f_row2_col6, #T_1b05f_row2_col7, #T_1b05f_row3_col0, #T_1b05f_row3_col1, #T_1b05f_row3_col4, #T_1b05f_row3_col5, #T_1b05f_row3_col6, #T_1b05f_row3_col7, #T_1b05f_row4_col0, #T_1b05f_row4_col1, #T_1b05f_row4_col2, #T_1b05f_row4_col4, #T_1b05f_row4_col5, #T_1b05f_row4_col6, #T_1b05f_row4_col7, #T_1b05f_row5_col0, #T_1b05f_row5_col1, #T_1b05f_row5_col2, #T_1b05f_row5_col3, #T_1b05f_row5_col4, #T_1b05f_row5_col5, #T_1b05f_row5_col6, #T_1b05f_row5_col7, #T_1b05f_row6_col0, #T_1b05f_row6_col1, #T_1b05f_row6_col2, #T_1b05f_row6_col3, #T_1b05f_row6_col4, #T_1b05f_row6_col5, #T_1b05f_row6_col6, #T_1b05f_row6_col7, #T_1b05f_row7_col0, #T_1b05f_row7_col1, #T_1b05f_row7_col2, #T_1b05f_row7_col3, #T_1b05f_row7_col4, #T_1b05f_row7_col5, #T_1b05f_row7_col6, #T_1b05f_row7_col7, #T_1b05f_row8_col0, #T_1b05f_row8_col1, #T_1b05f_row8_col2, #T_1b05f_row8_col3, #T_1b05f_row8_col4, #T_1b05f_row8_col5, #T_1b05f_row8_col6, #T_1b05f_row8_col7, #T_1b05f_row9_col0, #T_1b05f_row9_col1, #T_1b05f_row9_col2, #T_1b05f_row9_col3, #T_1b05f_row9_col4, #T_1b05f_row9_col5, #T_1b05f_row9_col6, #T_1b05f_row9_col7, #T_1b05f_row10_col0, #T_1b05f_row10_col1, #T_1b05f_row10_col2, #T_1b05f_row10_col3, #T_1b05f_row10_col4, #T_1b05f_row10_col5, #T_1b05f_row10_col6, #T_1b05f_row10_col7, #T_1b05f_row11_col0, #T_1b05f_row11_col1, #T_1b05f_row11_col2, #T_1b05f_row11_col3, #T_1b05f_row11_col4, #T_1b05f_row11_col5, #T_1b05f_row11_col6, #T_1b05f_row11_col7, #T_1b05f_row12_col0, #T_1b05f_row12_col1, #T_1b05f_row12_col2, #T_1b05f_row12_col3, #T_1b05f_row12_col4, #T_1b05f_row12_col5, #T_1b05f_row12_col6, #T_1b05f_row12_col7, #T_1b05f_row13_col0, #T_1b05f_row13_col1, #T_1b05f_row13_col2, #T_1b05f_row13_col3, #T_1b05f_row13_col4, #T_1b05f_row13_col5, #T_1b05f_row13_col6, #T_1b05f_row13_col7 {\n","  text-align: left;\n","}\n","#T_1b05f_row0_col1, #T_1b05f_row0_col2, #T_1b05f_row0_col3, #T_1b05f_row0_col4, #T_1b05f_row0_col5, #T_1b05f_row0_col6, #T_1b05f_row0_col7, #T_1b05f_row1_col1, #T_1b05f_row1_col2, #T_1b05f_row1_col3, #T_1b05f_row1_col4, #T_1b05f_row1_col5, #T_1b05f_row1_col6, #T_1b05f_row1_col7, #T_1b05f_row2_col3, #T_1b05f_row3_col2, #T_1b05f_row3_col3, #T_1b05f_row4_col3 {\n","  text-align: left;\n","  background-color: yellow;\n","}\n","#T_1b05f_row0_col8, #T_1b05f_row1_col8, #T_1b05f_row2_col8, #T_1b05f_row3_col8, #T_1b05f_row4_col8, #T_1b05f_row5_col8, #T_1b05f_row6_col8, #T_1b05f_row7_col8, #T_1b05f_row8_col8, #T_1b05f_row9_col8, #T_1b05f_row10_col8, #T_1b05f_row11_col8, #T_1b05f_row12_col8 {\n","  text-align: left;\n","  background-color: lightgrey;\n","}\n","#T_1b05f_row13_col8 {\n","  text-align: left;\n","  background-color: yellow;\n","  background-color: lightgrey;\n","}\n","</style>\n","<table id=\"T_1b05f\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_1b05f_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n","      <th id=\"T_1b05f_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n","      <th id=\"T_1b05f_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n","      <th id=\"T_1b05f_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n","      <th id=\"T_1b05f_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n","      <th id=\"T_1b05f_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n","      <th id=\"T_1b05f_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n","      <th id=\"T_1b05f_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n","      <th id=\"T_1b05f_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row0\" class=\"row_heading level0 row0\" >rf</th>\n","      <td id=\"T_1b05f_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n","      <td id=\"T_1b05f_row0_col1\" class=\"data row0 col1\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col2\" class=\"data row0 col2\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col3\" class=\"data row0 col3\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col4\" class=\"data row0 col4\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col5\" class=\"data row0 col5\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col6\" class=\"data row0 col6\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col7\" class=\"data row0 col7\" >1.0000</td>\n","      <td id=\"T_1b05f_row0_col8\" class=\"data row0 col8\" >23.7770</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row1\" class=\"row_heading level0 row1\" >et</th>\n","      <td id=\"T_1b05f_row1_col0\" class=\"data row1 col0\" >Extra Trees Classifier</td>\n","      <td id=\"T_1b05f_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col2\" class=\"data row1 col2\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col3\" class=\"data row1 col3\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col4\" class=\"data row1 col4\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col5\" class=\"data row1 col5\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col6\" class=\"data row1 col6\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col7\" class=\"data row1 col7\" >1.0000</td>\n","      <td id=\"T_1b05f_row1_col8\" class=\"data row1 col8\" >5.6510</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row2\" class=\"row_heading level0 row2\" >dt</th>\n","      <td id=\"T_1b05f_row2_col0\" class=\"data row2 col0\" >Decision Tree Classifier</td>\n","      <td id=\"T_1b05f_row2_col1\" class=\"data row2 col1\" >0.9984</td>\n","      <td id=\"T_1b05f_row2_col2\" class=\"data row2 col2\" >0.9984</td>\n","      <td id=\"T_1b05f_row2_col3\" class=\"data row2 col3\" >1.0000</td>\n","      <td id=\"T_1b05f_row2_col4\" class=\"data row2 col4\" >0.9967</td>\n","      <td id=\"T_1b05f_row2_col5\" class=\"data row2 col5\" >0.9984</td>\n","      <td id=\"T_1b05f_row2_col6\" class=\"data row2 col6\" >0.9967</td>\n","      <td id=\"T_1b05f_row2_col7\" class=\"data row2 col7\" >0.9967</td>\n","      <td id=\"T_1b05f_row2_col8\" class=\"data row2 col8\" >2.7150</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n","      <td id=\"T_1b05f_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n","      <td id=\"T_1b05f_row3_col1\" class=\"data row3 col1\" >0.9980</td>\n","      <td id=\"T_1b05f_row3_col2\" class=\"data row3 col2\" >1.0000</td>\n","      <td id=\"T_1b05f_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n","      <td id=\"T_1b05f_row3_col4\" class=\"data row3 col4\" >0.9960</td>\n","      <td id=\"T_1b05f_row3_col5\" class=\"data row3 col5\" >0.9980</td>\n","      <td id=\"T_1b05f_row3_col6\" class=\"data row3 col6\" >0.9960</td>\n","      <td id=\"T_1b05f_row3_col7\" class=\"data row3 col7\" >0.9960</td>\n","      <td id=\"T_1b05f_row3_col8\" class=\"data row3 col8\" >1.7830</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row4\" class=\"row_heading level0 row4\" >knn</th>\n","      <td id=\"T_1b05f_row4_col0\" class=\"data row4 col0\" >K Neighbors Classifier</td>\n","      <td id=\"T_1b05f_row4_col1\" class=\"data row4 col1\" >0.9965</td>\n","      <td id=\"T_1b05f_row4_col2\" class=\"data row4 col2\" >0.9989</td>\n","      <td id=\"T_1b05f_row4_col3\" class=\"data row4 col3\" >1.0000</td>\n","      <td id=\"T_1b05f_row4_col4\" class=\"data row4 col4\" >0.9930</td>\n","      <td id=\"T_1b05f_row4_col5\" class=\"data row4 col5\" >0.9965</td>\n","      <td id=\"T_1b05f_row4_col6\" class=\"data row4 col6\" >0.9930</td>\n","      <td id=\"T_1b05f_row4_col7\" class=\"data row4 col7\" >0.9930</td>\n","      <td id=\"T_1b05f_row4_col8\" class=\"data row4 col8\" >13.1880</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row5\" class=\"row_heading level0 row5\" >gbc</th>\n","      <td id=\"T_1b05f_row5_col0\" class=\"data row5 col0\" >Gradient Boosting Classifier</td>\n","      <td id=\"T_1b05f_row5_col1\" class=\"data row5 col1\" >0.9481</td>\n","      <td id=\"T_1b05f_row5_col2\" class=\"data row5 col2\" >0.9804</td>\n","      <td id=\"T_1b05f_row5_col3\" class=\"data row5 col3\" >0.9734</td>\n","      <td id=\"T_1b05f_row5_col4\" class=\"data row5 col4\" >0.9265</td>\n","      <td id=\"T_1b05f_row5_col5\" class=\"data row5 col5\" >0.9494</td>\n","      <td id=\"T_1b05f_row5_col6\" class=\"data row5 col6\" >0.8961</td>\n","      <td id=\"T_1b05f_row5_col7\" class=\"data row5 col7\" >0.8973</td>\n","      <td id=\"T_1b05f_row5_col8\" class=\"data row5 col8\" >54.0430</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row6\" class=\"row_heading level0 row6\" >ada</th>\n","      <td id=\"T_1b05f_row6_col0\" class=\"data row6 col0\" >Ada Boost Classifier</td>\n","      <td id=\"T_1b05f_row6_col1\" class=\"data row6 col1\" >0.8133</td>\n","      <td id=\"T_1b05f_row6_col2\" class=\"data row6 col2\" >0.9047</td>\n","      <td id=\"T_1b05f_row6_col3\" class=\"data row6 col3\" >0.7886</td>\n","      <td id=\"T_1b05f_row6_col4\" class=\"data row6 col4\" >0.8295</td>\n","      <td id=\"T_1b05f_row6_col5\" class=\"data row6 col5\" >0.8085</td>\n","      <td id=\"T_1b05f_row6_col6\" class=\"data row6 col6\" >0.6266</td>\n","      <td id=\"T_1b05f_row6_col7\" class=\"data row6 col7\" >0.6274</td>\n","      <td id=\"T_1b05f_row6_col8\" class=\"data row6 col8\" >11.6500</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row7\" class=\"row_heading level0 row7\" >svm</th>\n","      <td id=\"T_1b05f_row7_col0\" class=\"data row7 col0\" >SVM - Linear Kernel</td>\n","      <td id=\"T_1b05f_row7_col1\" class=\"data row7 col1\" >0.7261</td>\n","      <td id=\"T_1b05f_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n","      <td id=\"T_1b05f_row7_col3\" class=\"data row7 col3\" >0.6245</td>\n","      <td id=\"T_1b05f_row7_col4\" class=\"data row7 col4\" >0.7847</td>\n","      <td id=\"T_1b05f_row7_col5\" class=\"data row7 col5\" >0.6946</td>\n","      <td id=\"T_1b05f_row7_col6\" class=\"data row7 col6\" >0.4522</td>\n","      <td id=\"T_1b05f_row7_col7\" class=\"data row7 col7\" >0.4625</td>\n","      <td id=\"T_1b05f_row7_col8\" class=\"data row7 col8\" >0.6490</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row8\" class=\"row_heading level0 row8\" >lr</th>\n","      <td id=\"T_1b05f_row8_col0\" class=\"data row8 col0\" >Logistic Regression</td>\n","      <td id=\"T_1b05f_row8_col1\" class=\"data row8 col1\" >0.7200</td>\n","      <td id=\"T_1b05f_row8_col2\" class=\"data row8 col2\" >0.8081</td>\n","      <td id=\"T_1b05f_row8_col3\" class=\"data row8 col3\" >0.6398</td>\n","      <td id=\"T_1b05f_row8_col4\" class=\"data row8 col4\" >0.7620</td>\n","      <td id=\"T_1b05f_row8_col5\" class=\"data row8 col5\" >0.6955</td>\n","      <td id=\"T_1b05f_row8_col6\" class=\"data row8 col6\" >0.4399</td>\n","      <td id=\"T_1b05f_row8_col7\" class=\"data row8 col7\" >0.4457</td>\n","      <td id=\"T_1b05f_row8_col8\" class=\"data row8 col8\" >1.3430</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row9\" class=\"row_heading level0 row9\" >ridge</th>\n","      <td id=\"T_1b05f_row9_col0\" class=\"data row9 col0\" >Ridge Classifier</td>\n","      <td id=\"T_1b05f_row9_col1\" class=\"data row9 col1\" >0.7193</td>\n","      <td id=\"T_1b05f_row9_col2\" class=\"data row9 col2\" >0.0000</td>\n","      <td id=\"T_1b05f_row9_col3\" class=\"data row9 col3\" >0.6494</td>\n","      <td id=\"T_1b05f_row9_col4\" class=\"data row9 col4\" >0.7550</td>\n","      <td id=\"T_1b05f_row9_col5\" class=\"data row9 col5\" >0.6982</td>\n","      <td id=\"T_1b05f_row9_col6\" class=\"data row9 col6\" >0.4386</td>\n","      <td id=\"T_1b05f_row9_col7\" class=\"data row9 col7\" >0.4430</td>\n","      <td id=\"T_1b05f_row9_col8\" class=\"data row9 col8\" >0.2060</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row10\" class=\"row_heading level0 row10\" >lda</th>\n","      <td id=\"T_1b05f_row10_col0\" class=\"data row10 col0\" >Linear Discriminant Analysis</td>\n","      <td id=\"T_1b05f_row10_col1\" class=\"data row10 col1\" >0.7193</td>\n","      <td id=\"T_1b05f_row10_col2\" class=\"data row10 col2\" >0.7954</td>\n","      <td id=\"T_1b05f_row10_col3\" class=\"data row10 col3\" >0.6494</td>\n","      <td id=\"T_1b05f_row10_col4\" class=\"data row10 col4\" >0.7550</td>\n","      <td id=\"T_1b05f_row10_col5\" class=\"data row10 col5\" >0.6982</td>\n","      <td id=\"T_1b05f_row10_col6\" class=\"data row10 col6\" >0.4386</td>\n","      <td id=\"T_1b05f_row10_col7\" class=\"data row10 col7\" >0.4429</td>\n","      <td id=\"T_1b05f_row10_col8\" class=\"data row10 col8\" >0.2750</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row11\" class=\"row_heading level0 row11\" >qda</th>\n","      <td id=\"T_1b05f_row11_col0\" class=\"data row11 col0\" >Quadratic Discriminant Analysis</td>\n","      <td id=\"T_1b05f_row11_col1\" class=\"data row11 col1\" >0.6596</td>\n","      <td id=\"T_1b05f_row11_col2\" class=\"data row11 col2\" >0.8079</td>\n","      <td id=\"T_1b05f_row11_col3\" class=\"data row11 col3\" >0.3646</td>\n","      <td id=\"T_1b05f_row11_col4\" class=\"data row11 col4\" >0.8893</td>\n","      <td id=\"T_1b05f_row11_col5\" class=\"data row11 col5\" >0.5171</td>\n","      <td id=\"T_1b05f_row11_col6\" class=\"data row11 col6\" >0.3192</td>\n","      <td id=\"T_1b05f_row11_col7\" class=\"data row11 col7\" >0.3953</td>\n","      <td id=\"T_1b05f_row11_col8\" class=\"data row11 col8\" >0.4250</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row12\" class=\"row_heading level0 row12\" >nb</th>\n","      <td id=\"T_1b05f_row12_col0\" class=\"data row12 col0\" >Naive Bayes</td>\n","      <td id=\"T_1b05f_row12_col1\" class=\"data row12 col1\" >0.6575</td>\n","      <td id=\"T_1b05f_row12_col2\" class=\"data row12 col2\" >0.7487</td>\n","      <td id=\"T_1b05f_row12_col3\" class=\"data row12 col3\" >0.3578</td>\n","      <td id=\"T_1b05f_row12_col4\" class=\"data row12 col4\" >0.8931</td>\n","      <td id=\"T_1b05f_row12_col5\" class=\"data row12 col5\" >0.5109</td>\n","      <td id=\"T_1b05f_row12_col6\" class=\"data row12 col6\" >0.3150</td>\n","      <td id=\"T_1b05f_row12_col7\" class=\"data row12 col7\" >0.3935</td>\n","      <td id=\"T_1b05f_row12_col8\" class=\"data row12 col8\" >0.2780</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1b05f_level0_row13\" class=\"row_heading level0 row13\" >dummy</th>\n","      <td id=\"T_1b05f_row13_col0\" class=\"data row13 col0\" >Dummy Classifier</td>\n","      <td id=\"T_1b05f_row13_col1\" class=\"data row13 col1\" >0.5000</td>\n","      <td id=\"T_1b05f_row13_col2\" class=\"data row13 col2\" >0.5000</td>\n","      <td id=\"T_1b05f_row13_col3\" class=\"data row13 col3\" >0.0000</td>\n","      <td id=\"T_1b05f_row13_col4\" class=\"data row13 col4\" >0.0000</td>\n","      <td id=\"T_1b05f_row13_col5\" class=\"data row13 col5\" >0.0000</td>\n","      <td id=\"T_1b05f_row13_col6\" class=\"data row13 col6\" >0.0000</td>\n","      <td id=\"T_1b05f_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n","      <td id=\"T_1b05f_row13_col8\" class=\"data row13 col8\" >0.1720</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x1ebe88ae800>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n","  master_display_.apply(\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion=&#x27;gini&#x27;, max_depth=None, max_features=&#x27;sqrt&#x27;,\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_samples_leaf=1,\n","                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n","                       oob_score=False, random_state=1311, verbose=0,\n","                       warm_start=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion=&#x27;gini&#x27;, max_depth=None, max_features=&#x27;sqrt&#x27;,\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_samples_leaf=1,\n","                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n","                       oob_score=False, random_state=1311, verbose=0,\n","                       warm_start=False)</pre></div> </div></div></div></div>"],"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='sqrt',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_samples_leaf=1,\n","                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n","                       oob_score=False, random_state=1311, verbose=0,\n","                       warm_start=False)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["compare_models()"]},{"cell_type":"markdown","metadata":{},"source":["### Modelling"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.243001Z","iopub.status.busy":"2024-03-17T06:46:29.242520Z","iopub.status.idle":"2024-03-17T06:46:29.354904Z","shell.execute_reply":"2024-03-17T06:46:29.353649Z","shell.execute_reply.started":"2024-03-17T06:46:29.242960Z"},"trusted":true},"outputs":[],"source":["# # Inputs\n","# X_inputs = tf.keras.Input(shape=(30,), name = 'X_input')\n","\n","# # X\n","# X_dense1 = tf.keras.layers.Dense(64, activation = 'relu', name = 'X_dense1')(X_inputs)\n","# X_dense2 = tf.keras.layers.Dense(64, activation = 'relu', name = 'X_dense2')(X_dense1)\n","\n","# # Output\n","# outputs = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output')(X_dense2)\n","\n","# model = tf.keras.Model(inputs=[X_inputs], outputs = outputs)\n","\n","# print(model.summary())\n","# tf.keras.utils.plot_model(model)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.357576Z","iopub.status.busy":"2024-03-17T06:46:29.356857Z","iopub.status.idle":"2024-03-17T06:46:29.370552Z","shell.execute_reply":"2024-03-17T06:46:29.369384Z","shell.execute_reply.started":"2024-03-17T06:46:29.357523Z"},"trusted":true},"outputs":[],"source":["# model.compile(\n","#     optimizer = 'adam',\n","#     loss = 'binary_crossentropy',\n","#     metrics = [\n","#         'accuracy',\n","#         tf.keras.metrics.AUC(name='auc')\n","#     ]\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:46:29.372135Z","iopub.status.busy":"2024-03-17T06:46:29.371611Z","iopub.status.idle":"2024-03-17T06:48:09.882633Z","shell.execute_reply":"2024-03-17T06:48:09.881507Z","shell.execute_reply.started":"2024-03-17T06:46:29.372105Z"},"trusted":true},"outputs":[],"source":["# history = model.fit(\n","#     X_train,\n","#     y_train,\n","#     validation_split = 0.2,\n","#     batch_size = 32,\n","#     epochs = 10,\n","# )"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='sqrt',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_samples_leaf=1,\n","                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n","                       oob_score=False, random_state=1311, verbose=0,\n","                       warm_start=False).fit(X_train, y_train)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Initiated</th>\n","      <td>. . . . . . . . . . . . . . . . . .</td>\n","      <td>13:01:44</td>\n","    </tr>\n","    <tr>\n","      <th>Status</th>\n","      <td>. . . . . . . . . . . . . . . . . .</td>\n","      <td>Searching Hyperparameters</td>\n","    </tr>\n","    <tr>\n","      <th>Estimator</th>\n","      <td>. . . . . . . . . . . . . . . . . .</td>\n","      <td>Random Forest Classifier</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                         \n","                                                                         \n","Initiated  . . . . . . . . . . . . . . . . . .                   13:01:44\n","Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n","Estimator  . . . . . . . . . . . . . . . . . .   Random Forest Classifier"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2e3e22e9b2948409128271794ff0cf9","version_major":2,"version_minor":0},"text/plain":["Processing:   0%|          | 0/7 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuned_rf \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globals_d[name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\classification\\functional.py:1208\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;129m@check_if_global_is_not_none\u001b[39m(\u001b[38;5;28mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1019\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1038\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \n\u001b[0;32m   1206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[38;5;241m.\u001b[39mtune_model(\n\u001b[0;32m   1209\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1210\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28mround\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m,\n\u001b[0;32m   1212\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m   1213\u001b[0m         custom_grid\u001b[38;5;241m=\u001b[39mcustom_grid,\n\u001b[0;32m   1214\u001b[0m         optimize\u001b[38;5;241m=\u001b[39moptimize,\n\u001b[0;32m   1215\u001b[0m         custom_scorer\u001b[38;5;241m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1216\u001b[0m         search_library\u001b[38;5;241m=\u001b[39msearch_library,\n\u001b[0;32m   1217\u001b[0m         search_algorithm\u001b[38;5;241m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1218\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[0;32m   1219\u001b[0m         early_stopping_max_iters\u001b[38;5;241m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1220\u001b[0m         choose_better\u001b[38;5;241m=\u001b[39mchoose_better,\n\u001b[0;32m   1221\u001b[0m         fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1222\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m   1223\u001b[0m         return_tuner\u001b[38;5;241m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1224\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1225\u001b[0m         tuner_verbose\u001b[38;5;241m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1226\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1227\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1228\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\classification\\oop.py:1558\u001b[0m, in \u001b[0;36mClassificationExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1369\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtune_model(\n\u001b[0;32m   1559\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1560\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;28mround\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m,\n\u001b[0;32m   1562\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m   1563\u001b[0m         custom_grid\u001b[38;5;241m=\u001b[39mcustom_grid,\n\u001b[0;32m   1564\u001b[0m         optimize\u001b[38;5;241m=\u001b[39moptimize,\n\u001b[0;32m   1565\u001b[0m         custom_scorer\u001b[38;5;241m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1566\u001b[0m         search_library\u001b[38;5;241m=\u001b[39msearch_library,\n\u001b[0;32m   1567\u001b[0m         search_algorithm\u001b[38;5;241m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1568\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[0;32m   1569\u001b[0m         early_stopping_max_iters\u001b[38;5;241m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1570\u001b[0m         choose_better\u001b[38;5;241m=\u001b[39mchoose_better,\n\u001b[0;32m   1571\u001b[0m         fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1572\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m   1573\u001b[0m         return_tuner\u001b[38;5;241m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1574\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1575\u001b[0m         tuner_verbose\u001b[38;5;241m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1576\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1577\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1578\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2680\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2672\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[0;32m   2673\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2674\u001b[0m         pycaret\u001b[38;5;241m.\u001b[39minternal\u001b[38;5;241m.\u001b[39mpatches\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2675\u001b[0m     ):\n\u001b[0;32m   2676\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[0;32m   2677\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2678\u001b[0m             pycaret\u001b[38;5;241m.\u001b[39minternal\u001b[38;5;241m.\u001b[39mpatches\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2679\u001b[0m         ):\n\u001b[1;32m-> 2680\u001b[0m             model_grid\u001b[38;5;241m.\u001b[39mfit(data_X, data_y, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2682\u001b[0m     model_grid\u001b[38;5;241m.\u001b[39mfit(data_X, data_y, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\badal\\anaconda3\\envs\\pycarrot\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["tuned_rf = tune_model(model)"]},{"cell_type":"markdown","metadata":{},"source":["### Results"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T06:48:09.884897Z","iopub.status.busy":"2024-03-17T06:48:09.884534Z","iopub.status.idle":"2024-03-17T06:48:11.526390Z","shell.execute_reply":"2024-03-17T06:48:11.525310Z","shell.execute_reply.started":"2024-03-17T06:48:09.884867Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 99.860%\n","     Test AUC: 0.500\n"]}],"source":["from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Test Accuracy: {:.3f}%\".format(accuracy * 100))\n","\n","# Calculate AUC score\n","auc_score = roc_auc_score(y_test, y_pred)\n","print(\"     Test AUC: {:.3f}\".format(auc_score))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7952856,"sourceId":72239,"sourceType":"competition"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
